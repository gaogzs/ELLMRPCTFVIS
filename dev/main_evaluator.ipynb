{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51d072f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "import json\n",
    "\n",
    "\n",
    "from config import ModelInfo\n",
    "from utils.loaders import PromptLoader, SchemaLoader, InputTemplateLoader\n",
    "\n",
    "cur_dir = \"C:/Users/Shavius/Documents/Uni/Year 4/Project/ELLMRPCTFVIS/dev\"\n",
    "prompt_dir = os.path.join(cur_dir, 'prompts')\n",
    "prompt_loader = PromptLoader(prompt_dir)\n",
    "schema_dir = os.path.join(cur_dir, 'schemas')\n",
    "schema_loader = SchemaLoader(schema_dir)\n",
    "input_template_dir = os.path.join(cur_dir, 'input_templates')\n",
    "input_template_loader = InputTemplateLoader(input_template_dir)\n",
    "data_dir = os.path.join(cur_dir, 'test_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b8d7dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for narrative processing\n",
    "def wrap_narrative(narrative):\n",
    "    if not \"(User:\" in narrative:\n",
    "        return narrative + \"\\n(User:[hidden])\"\n",
    "    return narrative\n",
    "\n",
    "def divide_long_narratives(narrative, threshold=1000, section_length=800):\n",
    "    sections = []\n",
    "    start = 0\n",
    "    at_least_one_section = False\n",
    "    while len(narrative) - start > threshold or not at_least_one_section:\n",
    "        at_least_one_section = True\n",
    "        step_length = section_length if (len(narrative) - start) > (section_length * 2) else start + (len(narrative) - start) // 2\n",
    "        fullstop_index = narrative.find('. ', start + step_length)\n",
    "        if fullstop_index != -1:\n",
    "            sections.append(wrap_narrative(narrative[start:fullstop_index + 1].strip()))\n",
    "            start = fullstop_index + 2\n",
    "        else:\n",
    "            sections.append(wrap_narrative(narrative[start:].strip()))\n",
    "            return sections\n",
    "    if start < len(narrative):\n",
    "        sections.append(wrap_narrative(narrative[start:].strip()))\n",
    "    \n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a83a8eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def baseline_evaluator(model_info, input_narrative):\n",
    "    input_template = input_template_loader.load(\"consistency_evaluator_baseline\")\n",
    "    message = input_template.format(target_story=input_narrative)\n",
    "    \n",
    "    if model_info.output_format() == \"json\":\n",
    "        bot = model_info.chatbot()(model_info.model(), \"\", schema_loader)\n",
    "        text_response, json_response = bot.get_structured_response(message, schema_key=\"consistency_evaluator_baseline\", record=False, temperature=0)\n",
    "        consistency_score = json_response[\"consistency\"]\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported output format: {model_info.output_format()}\")\n",
    "\n",
    "    return consistency_score\n",
    "\n",
    "def run_baseline_per_chunk(model_info, dataset):\n",
    "    index_list = dataset.index.tolist()\n",
    "    out_data = []\n",
    "    for index in index_list:\n",
    "        input_narrative = dataset.loc[index, \"narrative\"]\n",
    "        consistency_score = baseline_evaluator(model_info, input_narrative)\n",
    "        out_data.append({\"narrative_id\": index, \"consistency\": consistency_score})\n",
    "        print(f\"Processed narrative {index}: Consistency score = {consistency_score}\")\n",
    "    return pd.DataFrame(out_data)\n",
    "\n",
    "async def run_baseline_async(model_info, dataset, out_dir, chunks=-1):\n",
    "    if os.path.exists(out_dir):\n",
    "        existing_data = pd.read_csv(out_dir)\n",
    "        dataset = dataset[~dataset.index.isin(existing_data[\"narrative_id\"].values)]\n",
    "        if dataset.empty:\n",
    "            print(\"No new narratives to process.\")\n",
    "            return\n",
    "    else:\n",
    "        existing_data = None\n",
    "    tasks = []\n",
    "    if chunks == -1:\n",
    "        chunk_size = 1\n",
    "    else:\n",
    "        chunk_size = len(dataset) // chunks if chunks > 0 else 1\n",
    "        \n",
    "    for i in range(0, len(dataset), chunk_size):\n",
    "        end_index = min(i + chunk_size, len(dataset))\n",
    "        chunk = dataset.iloc[i:end_index].copy()\n",
    "        task = asyncio.to_thread(run_baseline_per_chunk, model_info, chunk)\n",
    "        tasks.append(task)\n",
    "    \n",
    "    results_df = await asyncio.gather(*tasks)\n",
    "    complete_df = pd.concat(results_df, ignore_index=True)\n",
    "    complete_df = pd.concat([existing_data, complete_df], ignore_index=True) if existing_data is not None else complete_df\n",
    "    complete_df.sort_values(by='narrative_id').to_csv(out_dir, index=False)\n",
    "\n",
    "async def run_baseline_with_model(model_name, input_dir, prefix):\n",
    "    input_dataset = pd.read_csv(input_dir, index_col=0)\n",
    "    out_dir = os.path.join(os.path.dirname(input_dir), f\"{prefix}_baseline_output_{model_name}.csv\")\n",
    "    model_info = ModelInfo(model_name)\n",
    "    await run_baseline_async(model_info, input_dataset, out_dir, chunks=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04d2e838",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fol_evaluator import FOLEvaluationSession\n",
    "from timeline_maker import TimelineMakerSession\n",
    "\n",
    "def run_fol_evaluator_one(model_info, input_narrative, index):\n",
    "    timeline_session = TimelineMakerSession(model_info, prompt_dir=prompt_dir, schema_dir=schema_dir, input_template_dir=input_template_dir)\n",
    "    fol_session = FOLEvaluationSession(model_info, prompt_dir=prompt_dir, schema_dir=schema_dir, input_template_dir=input_template_dir)\n",
    "    divided_narratives = divide_long_narratives(input_narrative)\n",
    "    all_unsat_formulas = set()\n",
    "    j = 0\n",
    "    for section in divided_narratives:\n",
    "        j += 1\n",
    "        timeline_session.append_conversation(section)\n",
    "        new_timeline = timeline_session.get_timeline()\n",
    "        unsat_formulas = fol_session.append_conversation(section, new_timeline=new_timeline)\n",
    "        all_unsat_formulas.update(unsat_formulas)\n",
    "        print(f\"Processed section of narrative {j}/{len(divided_narratives)} of narrative {index}: Unsat formulas = {len(all_unsat_formulas)}\")\n",
    "    return all_unsat_formulas\n",
    "\n",
    "def run_fol_evaluator_per_chunk(model_info, dataset):\n",
    "    index_list = dataset.index.tolist()\n",
    "    out_data = []\n",
    "    for index in index_list:\n",
    "        processed_success = False\n",
    "        retry_count = 3\n",
    "        print(f\"Start processing {index}\")\n",
    "        while not processed_success:\n",
    "            try:\n",
    "                all_unsat_formulas = run_fol_evaluator_one(model_info, dataset.loc[index, \"narrative\"], index)\n",
    "                all_unsat_formulas_str = \"\\n\\n\".join(list(all_unsat_formulas)) if len(all_unsat_formulas) > 0 else \"No Output\"\n",
    "                processed_success = True\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing narrative {index}: {e}\")\n",
    "                retry_count -= 1\n",
    "                if retry_count <= 0:\n",
    "                    print(f\"Failed to process narrative {index} after multiple attempts.\")\n",
    "                    exit(1)\n",
    "        out_data.append({\"narrative_id\": index, \"unsat_formulas\": all_unsat_formulas_str})\n",
    "        print(f\"Processed narrative {index}: Unsat formulas = {len(all_unsat_formulas)}\")\n",
    "    return pd.DataFrame(out_data)\n",
    "\n",
    "async def run_fol_evaluator_async(model_info, dataset, out_dir, chunks=-1):\n",
    "    if os.path.exists(out_dir):\n",
    "        existing_data = pd.read_csv(out_dir)\n",
    "        dataset = dataset[~dataset.index.isin(existing_data[\"narrative_id\"].values)]\n",
    "        if dataset.empty:\n",
    "            print(\"No new narratives to process.\")\n",
    "            return\n",
    "    else:\n",
    "        existing_data = None\n",
    "    tasks = []\n",
    "    if chunks == -1:\n",
    "        chunk_size = 1\n",
    "    else:\n",
    "        chunk_size = len(dataset) // chunks if chunks > 0 else 1\n",
    "        \n",
    "    for i in range(0, len(dataset), chunk_size):\n",
    "        end_index = min(i + chunk_size, len(dataset))\n",
    "        chunk = dataset.iloc[i:end_index].copy()\n",
    "        task = asyncio.to_thread(run_fol_evaluator_per_chunk, model_info, chunk)\n",
    "        tasks.append(task)\n",
    "    \n",
    "    results_df = await asyncio.gather(*tasks)\n",
    "    complete_df = pd.concat(results_df, ignore_index=True)\n",
    "    complete_df = pd.concat([existing_data, complete_df], ignore_index=True) if existing_data is not None else complete_df\n",
    "    complete_df.sort_values(by='narrative_id').to_csv(out_dir, index=False)\n",
    "\n",
    "async def run_fol_evaluator_with_model(model_name, input_dir, prefix):\n",
    "    input_dataset = pd.read_csv(input_dir, index_col=0)\n",
    "    out_dir = os.path.join(os.path.dirname(input_dir), f\"{prefix}_fol_output_{model_name}.csv\")\n",
    "    model_info = ModelInfo(model_name)\n",
    "    await run_fol_evaluator_async(model_info, input_dataset, out_dir, chunks=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7c62e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shavius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from outline_evaluator import OutlineEvaluationSession\n",
    "\n",
    "def run_outline_evaluator_one(model_info, input_narrative, index):\n",
    "    outline_session = OutlineEvaluationSession(model_info, None, prompt_dir=prompt_dir, schema_dir=schema_dir, input_template_dir=input_template_dir)\n",
    "    divided_narratives = divide_long_narratives(input_narrative)\n",
    "    all_scores = {\"abruptness\": [], \"predicability\": []}\n",
    "    j = 0\n",
    "    for section in divided_narratives:\n",
    "        j += 1\n",
    "        new_scores = outline_session.append_conversation(section)\n",
    "        all_scores[\"abruptness\"].append(new_scores[\"abruptness\"])\n",
    "        all_scores[\"predicability\"].append(new_scores[\"predicability\"])\n",
    "        print(f\"Processed section {j}/{len(divided_narratives)} of narrative {index}: Scores = {new_scores}\")\n",
    "    return all_scores\n",
    "\n",
    "def run_outline_evaluator_per_chunk(model_info, dataset):\n",
    "    index_list = dataset.index.tolist()\n",
    "    out_data = []\n",
    "    for index in index_list:\n",
    "        processed_success = False\n",
    "        retry_count = 3\n",
    "        print(f\"Start processing {index}\")\n",
    "        while not processed_success:\n",
    "            try:\n",
    "                all_scores = run_outline_evaluator_one(model_info, dataset.loc[index, \"narrative\"], index)\n",
    "                processed_success = True\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing narrative {index}: {e}\")\n",
    "                retry_count -= 1\n",
    "                if retry_count <= 0:\n",
    "                    print(f\"Failed to process narrative {index} after multiple attempts.\")\n",
    "                    exit(1)\n",
    "        out_data.append({\"narrative_id\": index, \"outline_scores\": str(all_scores)})\n",
    "        print(f\"Processed narrative {index}: Scores = {all_scores}\")\n",
    "    return pd.DataFrame(out_data)\n",
    "\n",
    "async def run_outline_evaluator_async(model_info, dataset, out_dir, chunks=-1):\n",
    "    if os.path.exists(out_dir):\n",
    "        existing_data = pd.read_csv(out_dir)\n",
    "        dataset = dataset[~dataset.index.isin(existing_data[\"narrative_id\"].values)]\n",
    "        if dataset.empty:\n",
    "            print(\"No new narratives to process.\")\n",
    "            return\n",
    "    else:\n",
    "        existing_data = None\n",
    "    tasks = []\n",
    "    if chunks == -1:\n",
    "        chunk_size = 1\n",
    "    else:\n",
    "        chunk_size = len(dataset) // chunks if chunks > 0 else 1\n",
    "        \n",
    "    for i in range(0, len(dataset), chunk_size):\n",
    "        end_index = min(i + chunk_size, len(dataset))\n",
    "        chunk = dataset.iloc[i:end_index].copy()\n",
    "        task = asyncio.to_thread(run_outline_evaluator_per_chunk, model_info, chunk)\n",
    "        tasks.append(task)\n",
    "    \n",
    "    results_df = await asyncio.gather(*tasks)\n",
    "    complete_df = pd.concat(results_df, ignore_index=True)\n",
    "    complete_df = pd.concat([existing_data, complete_df], ignore_index=True) if existing_data is not None else complete_df\n",
    "    complete_df.sort_values(by='narrative_id').to_csv(out_dir, index=False)\n",
    "\n",
    "async def run_outline_evaluator_with_model(model_name, input_dir, prefix):\n",
    "    input_dataset = pd.read_csv(input_dir, index_col=0)\n",
    "    out_dir = os.path.join(os.path.dirname(input_dir), f\"{prefix}_outline_output_{model_name}.csv\")\n",
    "    model_info = ModelInfo(model_name)\n",
    "    await run_outline_evaluator_async(model_info, input_dataset, out_dir, chunks=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6de2f17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from character_evaluator import CharacterEvaluationSession\n",
    "\n",
    "def run_character_evaluator_one(model_info, input_narrative, index):\n",
    "    character_session = CharacterEvaluationSession(model_info, prompt_dir=prompt_dir, schema_dir=schema_dir, input_template_dir=input_template_dir)\n",
    "    divided_narratives = divide_long_narratives(input_narrative)\n",
    "    character_scores = {}\n",
    "    j = 0\n",
    "    for section in divided_narratives:\n",
    "        j += 1\n",
    "        new_scores = character_session.append_conversation(section)\n",
    "        for name, score in new_scores.items():\n",
    "            if name not in character_scores:\n",
    "                character_scores[name] = {\"self_integrity\": [], \"action_integrity\": []}\n",
    "            character_scores[name][\"self_integrity\"].append(score[\"self_integrity\"])\n",
    "            character_scores[name][\"action_integrity\"].append(score[\"action_integrity\"])\n",
    "        print(f\"Processed section {j}/{len(divided_narratives)} of narrative {index}: Scores = {new_scores}\")\n",
    "    return character_scores\n",
    "\n",
    "def run_character_evaluator_per_chunk(model_info, dataset):\n",
    "    index_list = dataset.index.tolist()\n",
    "    out_data = []\n",
    "    for index in index_list:\n",
    "        processed_success = False\n",
    "        retry_count = 3\n",
    "        print(f\"Start processing {index}\")\n",
    "        while not processed_success:\n",
    "            try:\n",
    "                character_scores = run_character_evaluator_one(model_info, dataset.loc[index, \"narrative\"], index)\n",
    "                processed_success = True\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing narrative {index}: {e}\")\n",
    "                retry_count -= 1\n",
    "                if retry_count <= 0:\n",
    "                    print(f\"Failed to process narrative {index} after multiple attempts.\")\n",
    "                    exit(1)\n",
    "        out_data.append({\"narrative_id\": index, \"character_scores\": json.dumps(character_scores, indent=2)})\n",
    "        print(f\"Processed narrative {index}: Scores = {character_scores}\")\n",
    "    return pd.DataFrame(out_data)\n",
    "\n",
    "async def run_character_evaluator_async(model_info, dataset, out_dir, chunks=-1):\n",
    "    if os.path.exists(out_dir):\n",
    "        existing_data = pd.read_csv(out_dir)\n",
    "        dataset = dataset[~dataset.index.isin(existing_data[\"narrative_id\"].values)]\n",
    "        if dataset.empty:\n",
    "            print(\"No new narratives to process.\")\n",
    "            return\n",
    "    else:\n",
    "        existing_data = None\n",
    "    tasks = []\n",
    "    if chunks == -1:\n",
    "        chunk_size = 1\n",
    "    else:\n",
    "        chunk_size = len(dataset) // chunks if chunks > 0 else 1\n",
    "        \n",
    "    for i in range(0, len(dataset), chunk_size):\n",
    "        end_index = min(i + chunk_size, len(dataset))\n",
    "        chunk = dataset.iloc[i:end_index].copy()\n",
    "        task = asyncio.to_thread(run_character_evaluator_per_chunk, model_info, chunk)\n",
    "        tasks.append(task)\n",
    "    \n",
    "    results_df = await asyncio.gather(*tasks)\n",
    "    complete_df = pd.concat(results_df, ignore_index=True)\n",
    "    complete_df = pd.concat([existing_data, complete_df], ignore_index=True) if existing_data is not None else complete_df\n",
    "    complete_df.sort_values(by='narrative_id').to_csv(out_dir, index=False)\n",
    "    \n",
    "async def run_character_evaluator_with_model(model_name, input_dir, prefix):\n",
    "    input_dataset = pd.read_csv(input_dir, index_col=0)\n",
    "    out_dir = os.path.join(os.path.dirname(input_dir), f\"{prefix}_character_output_{model_name}.csv\")\n",
    "    model_info = ModelInfo(model_name)\n",
    "    await run_character_evaluator_async(model_info, input_dataset, out_dir, chunks=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b6e917d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_evaluator(model_info, input_narrative, outline_result, character_result, fol_result):\n",
    "    input_template = input_template_loader.load(\"consistency_evaluator_combined\")\n",
    "    message = input_template.format(target_story=input_narrative, outline_evaluator_result=outline_result, character_evaluator_result=character_result, logical_evaluator_result=fol_result)\n",
    "    \n",
    "    if model_info.output_format() == \"json\":\n",
    "        bot = model_info.chatbot()(model_info.model(), \"\", schema_loader)\n",
    "        text_response, json_response = bot.get_structured_response(message, schema_key=\"consistency_evaluator_combined\", record=False, temperature=0)\n",
    "        consistency_score = json_response[\"consistency\"]\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported output format: {model_info.output_format()}\")\n",
    "    \n",
    "    return consistency_score\n",
    "\n",
    "def run_combined_evaluator_per_chunk(model_info, dataset):\n",
    "    index_list = dataset.index.tolist()\n",
    "    out_data = []\n",
    "    for index in index_list:\n",
    "        input_narrative = dataset.loc[index, \"narrative\"]\n",
    "        outline_result = dataset.loc[index, \"outline_scores\"]\n",
    "        character_result = dataset.loc[index, \"character_scores\"]\n",
    "        fol_result = dataset.loc[index, \"unsat_formulas\"]\n",
    "        consistency_score = combined_evaluator(model_info, input_narrative, outline_result, character_result, fol_result)\n",
    "        out_data.append({\"narrative_id\": index, \"consistency\": consistency_score})\n",
    "        print(f\"Processed narrative {index}: Consistency score = {consistency_score}\")\n",
    "    \n",
    "    return pd.DataFrame(out_data)\n",
    "\n",
    "async def run_combined_evaluator_async(model_info, dataset, out_dir, chunks=-1):\n",
    "    if os.path.exists(out_dir):\n",
    "        existing_data = pd.read_csv(out_dir)\n",
    "        dataset = dataset[~dataset.index.isin(existing_data[\"narrative_id\"].values)]\n",
    "        if dataset.empty:\n",
    "            print(\"No new narratives to process.\")\n",
    "            return\n",
    "    else:\n",
    "        existing_data = None\n",
    "    tasks = []\n",
    "    if chunks == -1:\n",
    "        chunk_size = 1\n",
    "    else:\n",
    "        chunk_size = len(dataset) // chunks if chunks > 0 else 1\n",
    "        \n",
    "    for i in range(0, len(dataset), chunk_size):\n",
    "        end_index = min(i + chunk_size, len(dataset))\n",
    "        chunk = dataset.iloc[i:end_index].copy()\n",
    "        task = asyncio.to_thread(run_combined_evaluator_per_chunk, model_info, chunk)\n",
    "        tasks.append(task)\n",
    "    \n",
    "    results_df = await asyncio.gather(*tasks)\n",
    "    complete_df = pd.concat(results_df, ignore_index=True)\n",
    "    complete_df = pd.concat([existing_data, complete_df], ignore_index=True) if existing_data is not None else complete_df\n",
    "    complete_df.sort_values(by='narrative_id').to_csv(out_dir, index=False)\n",
    "    \n",
    "async def run_combined_evaluator_with_model(model_name, input_dir, prefix):\n",
    "    input_dataset = pd.read_csv(input_dir, index_col=0)\n",
    "    \n",
    "    outline_dir = os.path.join(os.path.dirname(input_dir), f\"{prefix}_outline_output_{model_name}.csv\")\n",
    "    outline_df = pd.read_csv(outline_dir, index_col=0)\n",
    "    input_dataset = input_dataset.merge(outline_df, left_index=True, right_index=True, how='left')\n",
    "    \n",
    "    character_dir = os.path.join(os.path.dirname(input_dir), f\"{prefix}_character_output_{model_name}.csv\")\n",
    "    character_df = pd.read_csv(character_dir, index_col=0)\n",
    "    input_dataset = input_dataset.merge(character_df, left_index=True, right_index=True, how='left')\n",
    "    \n",
    "    fol_dir = os.path.join(os.path.dirname(input_dir), f\"{prefix}_fol_output_{model_name}.csv\")\n",
    "    fol_df = pd.read_csv(fol_dir, index_col=0)\n",
    "    input_dataset = input_dataset.merge(fol_df, left_index=True, right_index=True, how='left')\n",
    "    \n",
    "    out_dir = os.path.join(os.path.dirname(input_dir), f\"{prefix}_combined_output_{model_name}.csv\")\n",
    "    model_info = ModelInfo(model_name)\n",
    "    \n",
    "    await run_combined_evaluator_async(model_info, input_dataset, out_dir, chunks=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ff58bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.stats import spearmanr, kendalltau\n",
    "from sklearn.utils import resample\n",
    "\n",
    "def calculate_metric_correlations(human_scores, candidate_base, candidate_refined, n_bootsraps=1000, significance=0.05, rounding=3):\n",
    "    \n",
    "    if len(human_scores) == 0 or len(candidate_base) == 0 or len(candidate_refined) == 0:\n",
    "        raise ValueError(\"Input series must not be empty.\")\n",
    "    \n",
    "    if len(human_scores) != len(candidate_base) or len(human_scores) != len(candidate_refined):\n",
    "        raise ValueError(f\"Input series must have the same length.{len(human_scores)} != {len(candidate_base)}, {len(human_scores)} != {len(candidate_refined)}\")\n",
    "    \n",
    "    base_corr, _ = spearmanr(human_scores, candidate_base)\n",
    "    refined_corr, _ = spearmanr(human_scores, candidate_refined)\n",
    "    \n",
    "    n_obervations = len(human_scores)\n",
    "    data_pairs = np.column_stack((human_scores, candidate_base, candidate_refined))\n",
    "    \n",
    "    bootstrap_base_corrs = []\n",
    "    bootstrap_refined_corrs = []\n",
    "    \n",
    "    for i in range(n_bootsraps):\n",
    "        sample = resample(data_pairs, n_samples=n_obervations, replace=True)\n",
    "        \n",
    "        boot_base_corr, _ = spearmanr(sample[:, 0], sample[:, 1])\n",
    "        bootstrap_base_corrs.append(boot_base_corr)\n",
    "        \n",
    "        boot_refined_corr, _ = spearmanr(sample[:, 0], sample[:, 2])\n",
    "        bootstrap_refined_corrs.append(boot_refined_corr)\n",
    "    \n",
    "    ci_lower_base = np.percentile(bootstrap_base_corrs, 100 * significance / 2)\n",
    "    ci_upper_base = np.percentile(bootstrap_base_corrs, 100 * (1 - significance / 2))\n",
    "    \n",
    "    ci_lower_refined = np.percentile(bootstrap_refined_corrs, 100 * significance / 2)\n",
    "    ci_upper_refined = np.percentile(bootstrap_refined_corrs, 100 * (1 - significance / 2))\n",
    "    \n",
    "    base_corr = round(base_corr, rounding)\n",
    "    ci_lower_base = round(ci_lower_base, rounding)\n",
    "    ci_upper_base = round(ci_upper_base, rounding)\n",
    "    refined_corr = round(refined_corr, rounding)\n",
    "    ci_lower_refined = round(ci_lower_refined, rounding)\n",
    "    ci_upper_refined = round(ci_upper_refined, rounding)\n",
    "    \n",
    "    return base_corr, (ci_lower_base, ci_upper_base), refined_corr, (ci_lower_refined, ci_upper_refined)\n",
    "    \n",
    "def evaluate_model_result(model_name, prefix):\n",
    "    human_scores = pd.read_csv(os.path.join(data_dir, f\"{prefix}_annotations_average.csv\"), index_col=0)[\"consistency\"].values\n",
    "    base_scores = pd.read_csv(os.path.join(data_dir, f\"{prefix}_baseline_output_{model_name}.csv\"), index_col=0)[\"consistency\"].values\n",
    "    refined_scores = pd.read_csv(os.path.join(data_dir, f\"{prefix}_combined_output_{model_name}.csv\"), index_col=0)[\"consistency\"].values\n",
    "    \n",
    "    base_corr, (base_lower_ci, base_upper_ci), refined_corr, (refined_lower_ci, refined_upper_ci) = calculate_metric_correlations(human_scores, base_scores, refined_scores, significance=0.1)\n",
    "\n",
    "    print(f\"Base model correlation: {base_corr} ({base_lower_ci}-{base_upper_ci})\")\n",
    "    print(f\"Refined model correlation: {refined_corr} ({refined_lower_ci}-{refined_upper_ci})\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e0eb31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model: gemini-structured\n",
      "No new narratives to process.\n",
      "Start processing 0\n",
      "Start processing 1\n",
      "Start processing 2\n",
      "Start processing 3\n",
      "Start processing 4\n",
      "Start processing 5\n",
      "Start processing 6\n",
      "Start processing 7\n",
      "Start processing 8\n",
      "Start processing 9\n",
      "Start processing 10\n",
      "Start processing 11\n",
      "Start processing 12\n",
      "Start processing 13\n",
      "Start processing 14\n",
      "Start processing 15\n",
      "Start processing 16\n",
      "Start processing 17\n",
      "Start processing 18\n",
      "Start processing 19\n",
      "Processed section of narrative 1/4 of narrative 11: Unsat formulas = 0\n",
      "Processed section of narrative 1/2 of narrative 4: Unsat formulas = 0\n",
      "Processed section of narrative 1/2 of narrative 17: Unsat formulas = 0\n",
      "Processed section of narrative 1/2 of narrative 6: Unsat formulas = 3\n",
      "Processed section of narrative 1/5 of narrative 8: Unsat formulas = 0\n",
      "Processed section of narrative 1/5 of narrative 7: Unsat formulas = 0\n",
      "Processed section of narrative 1/2 of narrative 9: Unsat formulas = 0\n",
      "Processed section of narrative 1/4 of narrative 12: Unsat formulas = 0\n",
      "Processed section of narrative 1/5 of narrative 16: Unsat formulas = 0\n",
      "Processed section of narrative 1/2 of narrative 19: Unsat formulas = 0\n",
      "Processed section of narrative 1/3 of narrative 5: Unsat formulas = 0\n",
      "Processed section of narrative 1/5 of narrative 2: Unsat formulas = 0\n",
      "Processed section of narrative 1/2 of narrative 1: Unsat formulas = 0\n",
      "Processed section of narrative 1/2 of narrative 0: Unsat formulas = 0\n",
      "Processed section of narrative 1/5 of narrative 3: Unsat formulas = 0\n",
      "Processed section of narrative 1/2 of narrative 10: Unsat formulas = 3\n",
      "Processed section of narrative 1/2 of narrative 13: Unsat formulas = 0\n",
      "Processed section of narrative 2/4 of narrative 11: Unsat formulas = 0\n",
      "Processed section of narrative 2/2 of narrative 17: Unsat formulas = 0\n",
      "Processed narrative 17: Unsat formulas = 0\n",
      "Start processing 20\n",
      "Processed section of narrative 1/2 of narrative 14: Unsat formulas = 0\n",
      "Processed section of narrative 1/4 of narrative 15: Unsat formulas = 0\n",
      "Processed section of narrative 1/2 of narrative 18: Unsat formulas = 0\n",
      "Processed section of narrative 2/5 of narrative 7: Unsat formulas = 0\n",
      "Processed section of narrative 2/4 of narrative 12: Unsat formulas = 0\n",
      "Processed section of narrative 2/2 of narrative 9: Unsat formulas = 0\n",
      "Processed narrative 9: Unsat formulas = 0\n",
      "Start processing 21\n",
      "Processed section of narrative 2/2 of narrative 6: Unsat formulas = 4\n",
      "Processed narrative 6: Unsat formulas = 4\n",
      "Start processing 22\n",
      "Processed section of narrative 2/2 of narrative 1: Unsat formulas = 2\n",
      "Processed narrative 1: Unsat formulas = 2\n",
      "Start processing 23\n",
      "Processed section of narrative 2/5 of narrative 8: Unsat formulas = 0\n",
      "Processed section of narrative 2/2 of narrative 19: Unsat formulas = 3\n",
      "Processed narrative 19: Unsat formulas = 3\n",
      "Start processing 24\n",
      "Processed section of narrative 3/4 of narrative 11: Unsat formulas = 0\n",
      "Processed section of narrative 2/3 of narrative 5: Unsat formulas = 0\n",
      "Processed section of narrative 2/5 of narrative 2: Unsat formulas = 0\n",
      "Processed section of narrative 2/2 of narrative 10: Unsat formulas = 3\n",
      "Processed narrative 10: Unsat formulas = 3\n",
      "Start processing 25\n",
      "Processed section of narrative 2/4 of narrative 15: Unsat formulas = 0\n",
      "Processed section of narrative 2/2 of narrative 0: Unsat formulas = 0\n",
      "Processed narrative 0: Unsat formulas = 0\n",
      "Start processing 26\n",
      "Processed section of narrative 2/5 of narrative 3: Unsat formulas = 4\n",
      "Error processing narrative 16: Boolean expression expected\n",
      "Processed section of narrative 2/2 of narrative 13: Unsat formulas = 0\n",
      "Processed narrative 13: Unsat formulas = 0\n",
      "Start processing 27\n",
      "Processed section of narrative 1/5 of narrative 20: Unsat formulas = 0\n",
      "Processed section of narrative 1/2 of narrative 22: Unsat formulas = 0\n",
      "Processed section of narrative 2/2 of narrative 4: Unsat formulas = 3\n",
      "Processed narrative 4: Unsat formulas = 3\n",
      "Start processing 28\n",
      "Processed section of narrative 2/2 of narrative 14: Unsat formulas = 0\n",
      "Processed narrative 14: Unsat formulas = 0\n",
      "Start processing 29\n",
      "Processed section of narrative 1/2 of narrative 26: Unsat formulas = 0\n",
      "Processed section of narrative 1/4 of narrative 24: Unsat formulas = 0\n",
      "Processed section of narrative 2/2 of narrative 18: Unsat formulas = 0\n",
      "Processed narrative 18: Unsat formulas = 0\n",
      "Start processing 30\n",
      "Processed section of narrative 1/3 of narrative 21: Unsat formulas = 0\n",
      "Processed section of narrative 1/2 of narrative 23: Unsat formulas = 0\n",
      "Processed section of narrative 1/5 of narrative 25: Unsat formulas = 0\n",
      "Processed section of narrative 3/5 of narrative 8: Unsat formulas = 0\n",
      "Processed section of narrative 3/4 of narrative 15: Unsat formulas = 0\n",
      "Processed section of narrative 3/4 of narrative 12: Unsat formulas = 0\n",
      "Processed section of narrative 1/5 of narrative 16: Unsat formulas = 0\n",
      "Processed section of narrative 3/5 of narrative 7: Unsat formulas = 3\n",
      "Processed section of narrative 3/3 of narrative 5: Unsat formulas = 0\n",
      "Processed narrative 5: Unsat formulas = 0\n",
      "Start processing 31\n",
      "Processed section of narrative 4/4 of narrative 11: Unsat formulas = 0\n",
      "Processed narrative 11: Unsat formulas = 0\n",
      "Start processing 32\n",
      "Processed section of narrative 3/5 of narrative 3: Unsat formulas = 4\n",
      "Processed section of narrative 1/2 of narrative 27: Unsat formulas = 0\n",
      "Processed section of narrative 2/5 of narrative 20: Unsat formulas = 0\n",
      "Processed section of narrative 2/2 of narrative 26: Unsat formulas = 0\n",
      "Processed narrative 26: Unsat formulas = 0\n",
      "Start processing 33\n",
      "Processed section of narrative 2/2 of narrative 22: Unsat formulas = 0\n",
      "Processed narrative 22: Unsat formulas = 0\n",
      "Start processing 34\n",
      "Processed section of narrative 1/2 of narrative 29: Unsat formulas = 0\n",
      "Processed section of narrative 1/3 of narrative 28: Unsat formulas = 0\n",
      "Processed section of narrative 1/2 of narrative 30: Unsat formulas = 0\n",
      "Processed section of narrative 2/4 of narrative 24: Unsat formulas = 0\n",
      "Processed section of narrative 2/5 of narrative 25: Unsat formulas = 0\n",
      "Processed section of narrative 2/3 of narrative 21: Unsat formulas = 0\n",
      "Processed section of narrative 1/5 of narrative 31: Unsat formulas = 3\n",
      "Processed section of narrative 4/5 of narrative 8: Unsat formulas = 0\n",
      "Processed section of narrative 3/5 of narrative 2: Unsat formulas = 0\n",
      "Processed section of narrative 4/4 of narrative 15: Unsat formulas = 0\n",
      "Processed narrative 15: Unsat formulas = 0\n",
      "Start processing 35\n",
      "Processed section of narrative 2/5 of narrative 16: Unsat formulas = 0\n",
      "Processed section of narrative 3/5 of narrative 20: Unsat formulas = 0\n",
      "Processed section of narrative 1/2 of narrative 32: Unsat formulas = 0\n",
      "Processed section of narrative 4/5 of narrative 7: Unsat formulas = 3\n",
      "Processed section of narrative 4/5 of narrative 3: Unsat formulas = 4\n",
      "Processed section of narrative 2/2 of narrative 30: Unsat formulas = 0\n",
      "Processed narrative 30: Unsat formulas = 0\n",
      "Start processing 36\n",
      "Processed section of narrative 3/4 of narrative 24: Unsat formulas = 0\n",
      "Processed section of narrative 1/3 of narrative 34: Unsat formulas = 0\n",
      "Processed section of narrative 2/2 of narrative 29: Unsat formulas = 0\n",
      "Processed narrative 29: Unsat formulas = 0\n",
      "Start processing 37\n",
      "Processed section of narrative 1/4 of narrative 33: Unsat formulas = 0\n",
      "Processed section of narrative 2/2 of narrative 23: Unsat formulas = 0\n",
      "Processed narrative 23: Unsat formulas = 0\n",
      "Start processing 38\n",
      "Processed section of narrative 2/2 of narrative 27: Unsat formulas = 0\n",
      "Processed narrative 27: Unsat formulas = 0\n",
      "Start processing 39\n",
      "Processed section of narrative 2/3 of narrative 28: Unsat formulas = 0\n",
      "Processed section of narrative 3/3 of narrative 21: Unsat formulas = 0\n",
      "Processed narrative 21: Unsat formulas = 0\n",
      "Start processing 40\n",
      "Processed section of narrative 5/5 of narrative 8: Unsat formulas = 1\n",
      "Processed narrative 8: Unsat formulas = 1\n",
      "Start processing 41\n",
      "Processed section of narrative 4/5 of narrative 20: Unsat formulas = 0\n",
      "Processed section of narrative 4/5 of narrative 2: Unsat formulas = 0\n",
      "Processed section of narrative 2/2 of narrative 32: Unsat formulas = 0\n",
      "Processed narrative 32: Unsat formulas = 0\n",
      "Start processing 42\n",
      "Processed section of narrative 2/5 of narrative 31: Unsat formulas = 3\n",
      "Processed section of narrative 3/5 of narrative 25: Unsat formulas = 0\n",
      "Processed section of narrative 1/2 of narrative 36: Unsat formulas = 0\n",
      "Error processing narrative 35: Context mismatch\n",
      "Processed section of narrative 4/4 of narrative 12: Unsat formulas = 0\n",
      "Processed narrative 12: Unsat formulas = 0\n",
      "Start processing 43\n",
      "Processed section of narrative 2/3 of narrative 34: Unsat formulas = 3\n",
      "Processed section of narrative 4/4 of narrative 24: Unsat formulas = 0\n",
      "Processed narrative 24: Unsat formulas = 0\n",
      "Start processing 44\n",
      "Processed section of narrative 5/5 of narrative 3: Unsat formulas = 4\n",
      "Processed narrative 3: Unsat formulas = 4\n",
      "Start processing 45\n",
      "Processed section of narrative 3/5 of narrative 16: Unsat formulas = 0\n",
      "Processed section of narrative 5/5 of narrative 7: Unsat formulas = 3\n",
      "Processed narrative 7: Unsat formulas = 3\n",
      "Start processing 46\n",
      "Processed section of narrative 1/5 of narrative 39: Unsat formulas = 0\n",
      "Processed section of narrative 3/3 of narrative 28: Unsat formulas = 0\n",
      "Processed narrative 28: Unsat formulas = 0\n",
      "Start processing 47\n",
      "Processed section of narrative 1/3 of narrative 38: Unsat formulas = 3\n",
      "Processed section of narrative 1/4 of narrative 42: Unsat formulas = 0\n",
      "Processed section of narrative 1/1 of narrative 41: Unsat formulas = 0\n",
      "Processed narrative 41: Unsat formulas = 0\n",
      "Start processing 48\n",
      "Processed section of narrative 5/5 of narrative 20: Unsat formulas = 0\n",
      "Processed narrative 20: Unsat formulas = 0\n",
      "Start processing 49\n",
      "Processed section of narrative 1/2 of narrative 35: Unsat formulas = 0\n",
      "Processed section of narrative 1/2 of narrative 44: Unsat formulas = 0\n",
      "Processed section of narrative 4/5 of narrative 25: Unsat formulas = 0\n",
      "Processed section of narrative 1/3 of narrative 46: Unsat formulas = 0\n",
      "Processed section of narrative 1/3 of narrative 45: Unsat formulas = 0\n",
      "Processed section of narrative 1/2 of narrative 40: Unsat formulas = 0\n",
      "Processed section of narrative 2/2 of narrative 36: Unsat formulas = 0\n",
      "Processed narrative 36: Unsat formulas = 0\n",
      "Start processing 50\n",
      "Processed section of narrative 3/3 of narrative 34: Unsat formulas = 3\n",
      "Processed narrative 34: Unsat formulas = 3\n",
      "Start processing 51\n",
      "Processed section of narrative 2/4 of narrative 33: Unsat formulas = 0\n",
      "Processed section of narrative 4/5 of narrative 16: Unsat formulas = 0\n",
      "Processed section of narrative 1/4 of narrative 48: Unsat formulas = 0\n",
      "Processed section of narrative 2/4 of narrative 42: Unsat formulas = 0\n",
      "Processed section of narrative 1/2 of narrative 49: Unsat formulas = 0\n",
      "Processed section of narrative 2/3 of narrative 38: Unsat formulas = 3\n",
      "Processed section of narrative 2/2 of narrative 35: Unsat formulas = 0\n",
      "Processed narrative 35: Unsat formulas = 0\n",
      "Start processing 52\n",
      "Processed section of narrative 2/2 of narrative 44: Unsat formulas = 0\n",
      "Processed narrative 44: Unsat formulas = 0\n",
      "Start processing 53\n",
      "Processed section of narrative 1/4 of narrative 50: Unsat formulas = 0\n",
      "Processed section of narrative 2/3 of narrative 45: Unsat formulas = 0\n",
      "Processed section of narrative 3/5 of narrative 31: Unsat formulas = 3\n"
     ]
    }
   ],
   "source": [
    "model_queue = [\n",
    "    \"deepseek-structured\",\n",
    "    \"gpt-structured\",\n",
    "    \"claude-sonnet-structured\"\n",
    "]\n",
    "evaluaing_dataset_name = \"hanna\"\n",
    "\n",
    "for model_name in model_queue:\n",
    "    print(f\"Evaluating model: {model_name}\")\n",
    "    input_dataset_dir = os.path.join(data_dir, f\"{evaluaing_dataset_name}_stories.csv\")\n",
    "    await run_baseline_with_model(model_name, os.path.join(data_dir, input_dataset_dir), evaluaing_dataset_name)\n",
    "    await run_fol_evaluator_with_model(model_name, os.path.join(data_dir, input_dataset_dir), evaluaing_dataset_name)\n",
    "    await run_outline_evaluator_with_model(model_name, os.path.join(data_dir, input_dataset_dir), evaluaing_dataset_name)\n",
    "    await run_character_evaluator_with_model(model_name, os.path.join(data_dir, input_dataset_dir), evaluaing_dataset_name)\n",
    "    await run_combined_evaluator_with_model(model_name, os.path.join(data_dir, input_dataset_dir), evaluaing_dataset_name)\n",
    "    evaluate_model_result(model_name, evaluaing_dataset_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
