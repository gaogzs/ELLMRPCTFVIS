{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51d072f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "import json\n",
    "\n",
    "\n",
    "from config import ModelInfo\n",
    "from utils.loaders import PromptLoader, SchemaLoader, InputTemplateLoader\n",
    "\n",
    "cur_dir = \"C:/Users/Shavius/Documents/Uni/Year 4/Project/ELLMRPCTFVIS/dev\"\n",
    "prompt_dir = os.path.join(cur_dir, 'prompts')\n",
    "prompt_loader = PromptLoader(prompt_dir)\n",
    "schema_dir = os.path.join(cur_dir, 'schemas')\n",
    "schema_loader = SchemaLoader(schema_dir)\n",
    "input_template_dir = os.path.join(cur_dir, 'input_templates')\n",
    "input_template_loader = InputTemplateLoader(input_template_dir)\n",
    "data_dir = os.path.join(cur_dir, 'test_data')\n",
    "backup_fol = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b8d7dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for narrative processing\n",
    "def wrap_narrative(narrative):\n",
    "    if not \"(User:\" in narrative:\n",
    "        return narrative + \"\\n(User:[hidden])\"\n",
    "    return narrative\n",
    "\n",
    "def divide_long_narratives(narrative, threshold=1000, section_length=800):\n",
    "    sections = []\n",
    "    start = 0\n",
    "    at_least_one_section = False\n",
    "    while len(narrative) - start > threshold or not at_least_one_section:\n",
    "        at_least_one_section = True\n",
    "        step_length = section_length if (len(narrative) - start) > (section_length * 2) else start + (len(narrative) - start) // 2\n",
    "        fullstop_index = narrative.find('. ', start + step_length)\n",
    "        if fullstop_index != -1:\n",
    "            sections.append(wrap_narrative(narrative[start:fullstop_index + 1].strip()))\n",
    "            start = fullstop_index + 2\n",
    "        else:\n",
    "            sections.append(wrap_narrative(narrative[start:].strip()))\n",
    "            return sections\n",
    "    if start < len(narrative):\n",
    "        sections.append(wrap_narrative(narrative[start:].strip()))\n",
    "    \n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a83a8eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def baseline_evaluator(model_info, input_narrative):\n",
    "    input_template = input_template_loader.load(\"consistency_evaluator_baseline\")\n",
    "    message = input_template.format(target_story=input_narrative)\n",
    "    \n",
    "    if model_info.output_format() == \"json\":\n",
    "        bot = model_info.chatbot()(model_info.model(), \"\", schema_loader)\n",
    "        text_response, json_response = bot.get_structured_response(message, schema_key=\"consistency_evaluator_baseline\", record=False, temperature=0)\n",
    "        consistency_score = json_response[\"consistency\"]\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported output format: {model_info.output_format()}\")\n",
    "\n",
    "    return consistency_score\n",
    "\n",
    "def run_baseline_per_chunk(model_info, dataset):\n",
    "    index_list = dataset.index.tolist()\n",
    "    out_data = []\n",
    "    for index in index_list:\n",
    "        input_narrative = dataset.loc[index, \"narrative\"]\n",
    "        consistency_score = baseline_evaluator(model_info, input_narrative)\n",
    "        out_data.append({\"narrative_id\": index, \"consistency\": consistency_score})\n",
    "        print(f\"Processed narrative {index}: Consistency score = {consistency_score}\")\n",
    "    return pd.DataFrame(out_data)\n",
    "\n",
    "async def run_baseline_async(model_info, dataset, out_dir, chunks=-1):\n",
    "    if os.path.exists(out_dir):\n",
    "        existing_data = pd.read_csv(out_dir)\n",
    "        dataset = dataset[~dataset.index.isin(existing_data[\"narrative_id\"].values)]\n",
    "        if dataset.empty:\n",
    "            print(\"No new narratives to process.\")\n",
    "            return\n",
    "    else:\n",
    "        existing_data = None\n",
    "    tasks = []\n",
    "    if chunks == -1:\n",
    "        chunk_size = 1\n",
    "    else:\n",
    "        chunk_size = len(dataset) // chunks if chunks > 0 else 1\n",
    "        \n",
    "    for i in range(0, len(dataset), chunk_size):\n",
    "        end_index = min(i + chunk_size, len(dataset))\n",
    "        chunk = dataset.iloc[i:end_index].copy()\n",
    "        task = asyncio.to_thread(run_baseline_per_chunk, model_info, chunk)\n",
    "        tasks.append(task)\n",
    "    \n",
    "    results_df = existing_data.copy() if existing_data is not None else pd.DataFrame()\n",
    "    for future_result in asyncio.as_completed(tasks):\n",
    "        result_df = await future_result\n",
    "        results_df = pd.concat([results_df, result_df], ignore_index=True)\n",
    "        \n",
    "        results_df = results_df.sort_values(by='narrative_id')\n",
    "        results_df.to_csv(out_dir, index=False)\n",
    "        print(f\"Overall progress: {len(results_df)}/{len(dataset)}\")\n",
    "\n",
    "async def run_baseline_with_model(model_name, input_dir, prefix):\n",
    "    input_dataset = pd.read_csv(input_dir, index_col=0)\n",
    "    out_dir = os.path.join(os.path.dirname(input_dir), f\"{prefix}_baseline_output_{model_name}.csv\")\n",
    "    model_info = ModelInfo(model_name)\n",
    "    await run_baseline_async(model_info, input_dataset, out_dir, chunks=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04d2e838",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fol_evaluator import FOLEvaluationSession\n",
    "from timeline_maker import TimelineMakerSession\n",
    "\n",
    "def run_fol_evaluator_one(model_info, input_narrative, index):\n",
    "    timeline_session = TimelineMakerSession(model_info, prompt_dir=prompt_dir, schema_dir=schema_dir, input_template_dir=input_template_dir)\n",
    "    fol_session = FOLEvaluationSession(model_info, prompt_dir=prompt_dir, schema_dir=schema_dir, input_template_dir=input_template_dir)\n",
    "    divided_narratives = divide_long_narratives(input_narrative)\n",
    "    all_unsat_formulas = set()\n",
    "    j = 0\n",
    "    for section in divided_narratives:\n",
    "        j += 1\n",
    "        timeline_session.append_conversation(section)\n",
    "        new_timeline = timeline_session.get_timeline()\n",
    "        unsat_formulas = fol_session.append_conversation(section, new_timeline=new_timeline)\n",
    "        all_unsat_formulas.update(unsat_formulas)\n",
    "        print(f\"Processed section of narrative {j}/{len(divided_narratives)} of narrative {index}: Unsat formulas = {len(all_unsat_formulas)}\")\n",
    "    return all_unsat_formulas\n",
    "\n",
    "def run_fol_evaluator_per_chunk(model_info, dataset):\n",
    "    index_list = dataset.index.tolist()\n",
    "    out_data = []\n",
    "    for index in index_list:\n",
    "        processed_success = False\n",
    "        retry_count = 10\n",
    "        print(f\"Start processing {index}\")\n",
    "        while not processed_success:\n",
    "            try:\n",
    "                all_unsat_formulas = run_fol_evaluator_one(model_info, dataset.loc[index, \"narrative\"], index)\n",
    "                all_unsat_formulas_str = \"\\n\\n\".join(list(all_unsat_formulas)) if len(all_unsat_formulas) > 0 else \"No Output\"\n",
    "                processed_success = True\n",
    "                if backup_fol:\n",
    "                    with open(os.path.join(data_dir, f\"backup_logs/{model_info.info_name()}_{index}_unsat_formulas.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "                        f.write(all_unsat_formulas_str)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing narrative {index}: {e}\")\n",
    "                retry_count -= 1\n",
    "                if retry_count <= 0:\n",
    "                    print(f\"Failed to process narrative {index} after multiple attempts.\")\n",
    "                    all_unsat_formulas_str = \"Error processing narrative\"\n",
    "                    all_unsat_formulas = set()\n",
    "                    break\n",
    "        out_data.append({\"narrative_id\": index, \"unsat_formulas\": all_unsat_formulas_str})\n",
    "        print(f\"Processed narrative {index}: Unsat formulas = {len(all_unsat_formulas)}\")\n",
    "    \n",
    "    return pd.DataFrame(out_data)\n",
    "\n",
    "async def run_fol_evaluator_async(model_info, dataset, out_dir, chunks=-1):\n",
    "    total_data_size = len(dataset)\n",
    "    if os.path.exists(out_dir):\n",
    "        existing_data = pd.read_csv(out_dir)\n",
    "        dataset = dataset[~dataset.index.isin(existing_data[\"narrative_id\"].values)]\n",
    "        if dataset.empty:\n",
    "            print(\"No new narratives to process.\")\n",
    "            return\n",
    "    else:\n",
    "        existing_data = None\n",
    "    tasks = []\n",
    "    if chunks == -1:\n",
    "        chunk_size = 1\n",
    "    else:\n",
    "        chunk_size = len(dataset) // chunks if chunks > 0 else 1\n",
    "        \n",
    "    for i in range(0, len(dataset), chunk_size):\n",
    "        end_index = min(i + chunk_size, len(dataset))\n",
    "        chunk = dataset.iloc[i:end_index].copy()\n",
    "        task = asyncio.to_thread(run_fol_evaluator_per_chunk, model_info, chunk)\n",
    "        tasks.append(task)\n",
    "    \n",
    "    results_df = existing_data.copy() if existing_data is not None else pd.DataFrame()\n",
    "    for future_result in asyncio.as_completed(tasks):\n",
    "        result_df = await future_result\n",
    "        results_df = pd.concat([results_df, result_df], ignore_index=True)\n",
    "        \n",
    "        results_df = results_df.sort_values(by='narrative_id')\n",
    "        results_df.to_csv(out_dir, index=False)\n",
    "        print(f\"Overall progress: {len(results_df)}/{total_data_size}\")\n",
    "    print(\"FOL run completed\")\n",
    "\n",
    "async def run_fol_evaluator_with_model(model_name, input_dir, prefix):\n",
    "    input_dataset = pd.read_csv(input_dir, index_col=0)\n",
    "    out_dir = os.path.join(os.path.dirname(input_dir), f\"{prefix}_fol_output_{model_name}.csv\")\n",
    "    model_info = ModelInfo(model_name)\n",
    "    await run_fol_evaluator_async(model_info, input_dataset, out_dir, chunks=-1)\n",
    "    \n",
    "def sort_fol_backup(model_name, index_list, prefix):\n",
    "    sorted_data = []\n",
    "    for index in index_list:\n",
    "        target_dir = os.path.join(data_dir, f\"backup_logs/{model_name}_{index}_unsat_formulas.txt\")\n",
    "        if os.path.exists(target_dir):\n",
    "            with open(os.path.join(target_dir), \"r\", encoding=\"utf-8\") as f:\n",
    "                unsat_formulas = f.read().strip()\n",
    "            sorted_data.append({\"narrative_id\": index, \"unsat_formulas\": unsat_formulas})\n",
    "    \n",
    "    pd.DataFrame(sorted_data).to_csv(os.path.join(data_dir, f\"backup_logs/prefix_fol_output_{model_name}_sorted.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7c62e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shavius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from outline_evaluator import OutlineEvaluationSession\n",
    "\n",
    "def run_outline_evaluator_one(model_info, input_narrative, index):\n",
    "    outline_session = OutlineEvaluationSession(model_info, None, prompt_dir=prompt_dir, schema_dir=schema_dir, input_template_dir=input_template_dir)\n",
    "    divided_narratives = divide_long_narratives(input_narrative)\n",
    "    all_scores = {\"abruptness\": [], \"predicability\": []}\n",
    "    j = 0\n",
    "    for section in divided_narratives:\n",
    "        j += 1\n",
    "        new_scores = outline_session.append_conversation(section)\n",
    "        all_scores[\"abruptness\"].append(new_scores[\"abruptness\"])\n",
    "        all_scores[\"predicability\"].append(new_scores[\"predicability\"])\n",
    "        print(f\"Processed section {j}/{len(divided_narratives)} of narrative {index}: Scores = {new_scores}\")\n",
    "    return all_scores\n",
    "\n",
    "def run_outline_evaluator_per_chunk(model_info, dataset):\n",
    "    index_list = dataset.index.tolist()\n",
    "    out_data = []\n",
    "    for index in index_list:\n",
    "        processed_success = False\n",
    "        retry_count = 10\n",
    "        print(f\"Start processing {index}\")\n",
    "        while not processed_success:\n",
    "            try:\n",
    "                all_scores = run_outline_evaluator_one(model_info, dataset.loc[index, \"narrative\"], index)\n",
    "                processed_success = True\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing narrative {index}: {e}\")\n",
    "                retry_count -= 1\n",
    "                if retry_count <= 0:\n",
    "                    print(f\"Failed to process narrative {index} after multiple attempts.\")\n",
    "                    all_scores = {\"abruptness\": [0], \"predicability\": [0.5]}\n",
    "                    break\n",
    "                \n",
    "        out_data.append({\"narrative_id\": index, \"outline_scores\": str(all_scores)})\n",
    "        print(f\"Processed narrative {index}: Scores = {all_scores}\")\n",
    "    return pd.DataFrame(out_data)\n",
    "\n",
    "async def run_outline_evaluator_async(model_info, dataset, out_dir, chunks=-1):\n",
    "    total_data_size = len(dataset)\n",
    "    if os.path.exists(out_dir):\n",
    "        existing_data = pd.read_csv(out_dir)\n",
    "        dataset = dataset[~dataset.index.isin(existing_data[\"narrative_id\"].values)]\n",
    "        if dataset.empty:\n",
    "            print(\"No new narratives to process.\")\n",
    "            return\n",
    "    else:\n",
    "        existing_data = None\n",
    "    tasks = []\n",
    "    if chunks == -1:\n",
    "        chunk_size = 1\n",
    "    else:\n",
    "        chunk_size = len(dataset) // chunks if chunks > 0 else 1\n",
    "        \n",
    "    for i in range(0, len(dataset), chunk_size):\n",
    "        end_index = min(i + chunk_size, len(dataset))\n",
    "        chunk = dataset.iloc[i:end_index].copy()\n",
    "        task = asyncio.to_thread(run_outline_evaluator_per_chunk, model_info, chunk)\n",
    "        tasks.append(task)\n",
    "    \n",
    "    results_df = existing_data.copy() if existing_data is not None else pd.DataFrame()\n",
    "    for future_result in asyncio.as_completed(tasks):\n",
    "        result_df = await future_result\n",
    "        results_df = pd.concat([results_df, result_df], ignore_index=True)\n",
    "        \n",
    "        results_df = results_df.sort_values(by='narrative_id')\n",
    "        results_df.to_csv(out_dir, index=False)\n",
    "        print(f\"Overall progress: {len(results_df)}/{total_data_size}\")\n",
    "    print(\"Outline run completed\")\n",
    "\n",
    "async def run_outline_evaluator_with_model(model_name, input_dir, prefix):\n",
    "    input_dataset = pd.read_csv(input_dir, index_col=0)\n",
    "    out_dir = os.path.join(os.path.dirname(input_dir), f\"{prefix}_outline_output_{model_name}.csv\")\n",
    "    model_info = ModelInfo(model_name)\n",
    "    await run_outline_evaluator_async(model_info, input_dataset, out_dir, chunks=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6de2f17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from character_evaluator import CharacterEvaluationSession\n",
    "\n",
    "def run_character_evaluator_one(model_info, input_narrative, index):\n",
    "    character_session = CharacterEvaluationSession(model_info, prompt_dir=prompt_dir, schema_dir=schema_dir, input_template_dir=input_template_dir)\n",
    "    divided_narratives = divide_long_narratives(input_narrative)\n",
    "    character_scores = {}\n",
    "    j = 0\n",
    "    for section in divided_narratives:\n",
    "        j += 1\n",
    "        new_scores = character_session.append_conversation(section)\n",
    "        for name, score in new_scores.items():\n",
    "            if name not in character_scores:\n",
    "                character_scores[name] = {\"self_integrity\": [], \"action_integrity\": []}\n",
    "            character_scores[name][\"self_integrity\"].append(score[\"self_integrity\"])\n",
    "            character_scores[name][\"action_integrity\"].append(score[\"action_integrity\"])\n",
    "        print(f\"Processed section {j}/{len(divided_narratives)} of narrative {index}: Scores = {new_scores}\")\n",
    "    return character_scores\n",
    "\n",
    "def run_character_evaluator_per_chunk(model_info, dataset):\n",
    "    index_list = dataset.index.tolist()\n",
    "    out_data = []\n",
    "    for index in index_list:\n",
    "        processed_success = False\n",
    "        retry_count = 10\n",
    "        print(f\"Start processing {index}\")\n",
    "        while not processed_success:\n",
    "            try:\n",
    "                character_scores = run_character_evaluator_one(model_info, dataset.loc[index, \"narrative\"], index)\n",
    "                processed_success = True\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing narrative {index}: {e}\")\n",
    "                retry_count -= 1\n",
    "                if retry_count <= 0:\n",
    "                    print(f\"Failed to process narrative {index} after multiple attempts.\")\n",
    "                    character_scores = {}\n",
    "                    break\n",
    "                \n",
    "        out_data.append({\"narrative_id\": index, \"character_scores\": json.dumps(character_scores, indent=2)})\n",
    "        print(f\"Processed narrative {index}: Scores = {character_scores}\")\n",
    "    return pd.DataFrame(out_data)\n",
    "\n",
    "async def run_character_evaluator_async(model_info, dataset, out_dir, chunks=-1):\n",
    "    total_data_size = len(dataset)\n",
    "    if os.path.exists(out_dir):\n",
    "        existing_data = pd.read_csv(out_dir)\n",
    "        dataset = dataset[~dataset.index.isin(existing_data[\"narrative_id\"].values)]\n",
    "        if dataset.empty:\n",
    "            print(\"No new narratives to process.\")\n",
    "            return\n",
    "    else:\n",
    "        existing_data = None\n",
    "    tasks = []\n",
    "    if chunks == -1:\n",
    "        chunk_size = 1\n",
    "    else:\n",
    "        chunk_size = len(dataset) // chunks if chunks > 0 else 1\n",
    "        \n",
    "    for i in range(0, len(dataset), chunk_size):\n",
    "        end_index = min(i + chunk_size, len(dataset))\n",
    "        chunk = dataset.iloc[i:end_index].copy()\n",
    "        task = asyncio.to_thread(run_character_evaluator_per_chunk, model_info, chunk)\n",
    "        tasks.append(task)\n",
    "    \n",
    "    results_df = existing_data.copy() if existing_data is not None else pd.DataFrame()\n",
    "    for future_result in asyncio.as_completed(tasks):\n",
    "        result_df = await future_result\n",
    "        results_df = pd.concat([results_df, result_df], ignore_index=True)\n",
    "        \n",
    "        results_df = results_df.sort_values(by='narrative_id')\n",
    "        results_df.to_csv(out_dir, index=False)\n",
    "        print(f\"Overall progress: {len(results_df)}/{total_data_size}\")\n",
    "    print(\"Character run completed\")\n",
    "    \n",
    "async def run_character_evaluator_with_model(model_name, input_dir, prefix):\n",
    "    input_dataset = pd.read_csv(input_dir, index_col=0)\n",
    "    out_dir = os.path.join(os.path.dirname(input_dir), f\"{prefix}_character_output_{model_name}.csv\")\n",
    "    model_info = ModelInfo(model_name)\n",
    "    await run_character_evaluator_async(model_info, input_dataset, out_dir, chunks=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6e917d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_evaluator(model_info, input_narrative, outline_result, character_result, fol_result):\n",
    "    input_template = input_template_loader.load(\"consistency_evaluator_combined\")\n",
    "    message = input_template.format(target_story=input_narrative, outline_evaluator_result=outline_result, character_evaluator_result=character_result, logical_evaluator_result=fol_result)\n",
    "    if model_info.output_format() == \"json\":\n",
    "        bot = model_info.chatbot()(model_info.model(), \"\", schema_loader)\n",
    "        text_response, json_response = bot.get_structured_response(message, schema_key=\"consistency_evaluator_combined\", record=False, temperature=0.1)\n",
    "        consistency_score = json_response[\"consistency\"]\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported output format: {model_info.output_format()}\")\n",
    "    \n",
    "    return consistency_score\n",
    "\n",
    "def run_combined_evaluator_per_chunk(model_info, dataset):\n",
    "    index_list = dataset.index.tolist()\n",
    "    out_data = []\n",
    "    for index in index_list:\n",
    "        input_narrative = dataset.loc[index, \"narrative\"]\n",
    "        outline_result = dataset.loc[index, \"outline_scores\"]\n",
    "        character_result = dataset.loc[index, \"character_scores\"]\n",
    "        fol_result = dataset.loc[index, \"unsat_formulas\"]\n",
    "        consistency_score = combined_evaluator(model_info, input_narrative, outline_result, character_result, fol_result)\n",
    "        out_data.append({\"narrative_id\": index, \"consistency\": consistency_score})\n",
    "        print(f\"Processed narrative {index}: Consistency score = {consistency_score}\")\n",
    "    \n",
    "    return pd.DataFrame(out_data)\n",
    "\n",
    "async def run_combined_evaluator_async(model_info, dataset, out_dir, chunks=-1):\n",
    "    total_data_size = len(dataset)\n",
    "    if os.path.exists(out_dir):\n",
    "        existing_data = pd.read_csv(out_dir)\n",
    "        dataset = dataset[~dataset.index.isin(existing_data[\"narrative_id\"].values)]\n",
    "        if dataset.empty:\n",
    "            print(\"No new narratives to process.\")\n",
    "            return\n",
    "    else:\n",
    "        existing_data = None\n",
    "    tasks = []\n",
    "    if chunks == -1:\n",
    "        chunk_size = 1\n",
    "    else:\n",
    "        chunk_size = len(dataset) // chunks if chunks > 0 else 1\n",
    "        \n",
    "    for i in range(0, len(dataset), chunk_size):\n",
    "        end_index = min(i + chunk_size, len(dataset))\n",
    "        chunk = dataset.iloc[i:end_index].copy()\n",
    "        task = asyncio.to_thread(run_combined_evaluator_per_chunk, model_info, chunk)\n",
    "        tasks.append(task)\n",
    "    \n",
    "    results_df = existing_data.copy() if existing_data is not None else pd.DataFrame()\n",
    "    for future_result in asyncio.as_completed(tasks):\n",
    "        result_df = await future_result\n",
    "        results_df = pd.concat([results_df, result_df], ignore_index=True)\n",
    "        \n",
    "        results_df = results_df.sort_values(by='narrative_id')\n",
    "        results_df.to_csv(out_dir, index=False)\n",
    "        print(f\"Overall progress: {len(results_df)}/{total_data_size}\")\n",
    "    print(\"Combined run completed\")\n",
    "    \n",
    "async def run_combined_evaluator_with_model(model_name, input_dir, prefix):\n",
    "    input_dataset = pd.read_csv(input_dir, index_col=0)\n",
    "    \n",
    "    outline_dir = os.path.join(os.path.dirname(input_dir), f\"{prefix}_outline_output_{model_name}.csv\")\n",
    "    outline_df = pd.read_csv(outline_dir, index_col=0)\n",
    "    input_dataset = input_dataset.merge(outline_df, left_index=True, right_index=True, how='left')\n",
    "    \n",
    "    character_dir = os.path.join(os.path.dirname(input_dir), f\"{prefix}_character_output_{model_name}.csv\")\n",
    "    character_df = pd.read_csv(character_dir, index_col=0)\n",
    "    input_dataset = input_dataset.merge(character_df, left_index=True, right_index=True, how='left')\n",
    "    \n",
    "    fol_dir = os.path.join(os.path.dirname(input_dir), f\"{prefix}_fol_output_{model_name}.csv\")\n",
    "    fol_df = pd.read_csv(fol_dir, index_col=0)\n",
    "    input_dataset = input_dataset.merge(fol_df, left_index=True, right_index=True, how='left')\n",
    "    \n",
    "    out_dir = os.path.join(os.path.dirname(input_dir), f\"{prefix}_combined_output_{model_name}.csv\")\n",
    "    model_info = ModelInfo(model_name)\n",
    "    \n",
    "    await run_combined_evaluator_async(model_info, input_dataset, out_dir, chunks=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82e0eb31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model: gemini-lite-structured\n",
      "No new narratives to process.\n",
      "No new narratives to process.\n",
      "No new narratives to process.\n",
      "No new narratives to process.\n",
      "Target Story:\n",
      "I have always liked trains, and it gives me time to think. It brings my memory in pieces and helps me recall things from a kid. Just a small drop of memory to recall, and a small little blip from when I was younger. I was bored, as always, I ended up on the wrong train. I had never seen a person before, but no one ever talked to me about them. Time slowed to a crawl. The sensation I had given up on when I was younger. You 'd do that. Is it good for you to have people watching, but not feel these kinds of signals or experiences, just feel them? How can one just pretend it's worth it? And yet, you keep going on. You keep walking, always going forwards, never even stopping, the same point that's been happening before, is today. Each train is different in its own way, and every memory is still bound to an destination. You will want to see it, but if you feel, if you continue to stare, perhaps you 'll get a few seconds of closure. Some people ignore the repeated history of their ancestors, but I keep looking and looking. The fog is filling the station, but the pain is pretty normal. An x-ray showed a man and woman, arm in arm. They are wearing a long, jagged\n",
      "\n",
      "Briefly interpret the story in your own language in the \"reasoning\" part and rate the story on a scale from 1 to 5 on its consistency (How well the story maintains a coherent narrative without contradictions or inconsistencies).\n",
      "\n",
      "**Sub-Agents Response**\n",
      "You will be working with 3 sub-agents, their output may help you.\n",
      "\n",
      "Outline Evaluator Result:\n",
      "{'abruptness': [0, -0.1], 'predicability': [1, -0.5]}\n",
      "Outline evaluator will respond with 2 metrics: abruptness and predicability. Each metric will come in a list, where each element is judging a proportional part of the story. Abruptness range from -1 to 1, where high value means the plot is changing to abruptly, and low value means the plot is sticking with the same direction for too long, a value around 0 suggest fine situation. Predicability range from -1 to 1+, it represents how not-surprising the plot is progressing, where high value means the plot progressing just as expected, and low value means the plot is progressing in a way that is not expected. A value within 0-1 is considered moderate situation, it can sometimes go above 1. Higher value means the plot is conherent, but lower does not necessarily mean it is bad.\n",
      "Briefly interpret the results in your own language and review the corresponding part of the story in the \"reasoning\" part.\n",
      "\n",
      "Character Evaluator Result:\n",
      "{\n",
      "  \"The Narrator\": {\n",
      "    \"self_integrity\": [\n",
      "      1.0,\n",
      "      0.8\n",
      "    ],\n",
      "    \"action_integrity\": [\n",
      "      0.8,\n",
      "      0.5\n",
      "    ]\n",
      "  },\n",
      "  \"The Man\": {\n",
      "    \"self_integrity\": [\n",
      "      1.0\n",
      "    ],\n",
      "    \"action_integrity\": [\n",
      "      1.0\n",
      "    ]\n",
      "  },\n",
      "  \"The Woman\": {\n",
      "    \"self_integrity\": [\n",
      "      1.0\n",
      "    ],\n",
      "    \"action_integrity\": [\n",
      "      1.0\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "Character evaluator will extract all characters from the story and evaluate each with 2 metrics: self_integrity and action_integrity. Each metric will come in a list, where each element is judging a proportional part of the story. Both values range from 0 to 1, where higher value means better consistency. self_integrity means how consistent is the character built, and action_integrity means how consistent is the character's actions with the built character.\n",
      "Briefly interpret the results in your own language and review the content of the story related to the corresponding character in the \"reasoning\" part.\n",
      "\n",
      "Logical Evaluator Result:\n",
      "(forall ((a Int) (b1 Int) (b2 Int) (t Int))\n",
      "  (=> (and (is_bound_to a b1 t) (is_bound_to a b2 t)) (= b1 b2)))\n",
      "\n",
      "(forall ((a Int) (b Int) (t Int)) (is_bound_to a b t))\n",
      "Logical evaluator will convert the story into a set of FOL formula and finds what elements might be contradicting each other. It will return a list of FOL propositions that migh be violated to you. If no output is given, it does not necessarily mean there is no contradiction, but the evaluator simply failed to find any. When you see some variable names like T0, T1, T2, they represents abstract time points, you may need to consider what might go wrong in the story that might cause chronological contradiction.\n",
      "Interpret all given formulas (if any) to natural language in the \"reasoning\" part, and review the content of the story related to the formulas.\n",
      "\n",
      "At last you need to give your reasons of rating the story in the \"reasoning\" part, based on the following detailed criteria:\n",
      "- 1: The story is completely incoherent and makes no sense at all, with numerous contradictions and inconsistencies.\n",
      "- 2: The story has numerous inconsistency problems, making it difficult to follow or understand.\n",
      "- 3: The story has some inconsistency problems causing a little confusion, but they do not significantly detract from the overall narrative.\n",
      "- 4: The story is mostly consistent, with only minor inconsistencies that do not affect the overall narrative.\n",
      "- 5: The story is completely consistent, with no contradictions or inconsistencies at all.\n",
      "\n",
      "Respond in json format with a single string and a single integer in:\n",
      "{\"reasoning\": <reasoning>, \"consistency\": <score>}\n",
      "\"{\n",
      "  \"reasoning\": \"The story is a stream of consciousness, filled with fragmented thoughts and memories triggered by trains. The narrator reflects on childhood experiences, feelings of isolation, and the passage of time. The narrative is disjointed, jumping between different ideas and moments without clear transitions. The Outline Evaluator indicates that the story's abruptness is around 0, suggesting a moderate pace of plot changes, and the predictability is low, indicating an unexpected plot progression. This aligns with the story's style. The Character Evaluator shows high integrity for all characters, but the story's focus is on the narrator's internal state, and the other characters are only mentioned briefly. The Logical Evaluator did not find any contradictions. However, the lack of clear narrative structure, the abstract nature of the thoughts, and the absence of a cohesive plot make it difficult to follow. The story's fragmented nature and lack of clear connections between ideas lead to a lack of coherence. Therefore, the story is rated low on consistency.\",\n",
      "  \"consistency\": 2\n",
      "}\"\n",
      "{'reasoning': \"The story is a stream of consciousness, filled with fragmented thoughts and memories triggered by trains. The narrator reflects on childhood experiences, feelings of isolation, and the passage of time. The narrative is disjointed, jumping between different ideas and moments without clear transitions. The Outline Evaluator indicates that the story's abruptness is around 0, suggesting a moderate pace of plot changes, and the predictability is low, indicating an unexpected plot progression. This aligns with the story's style. The Character Evaluator shows high integrity for all characters, but the story's focus is on the narrator's internal state, and the other characters are only mentioned briefly. The Logical Evaluator did not find any contradictions. However, the lack of clear narrative structure, the abstract nature of the thoughts, and the absence of a cohesive plot make it difficult to follow. The story's fragmented nature and lack of clear connections between ideas lead to a lack of coherence. Therefore, the story is rated low on consistency.\", 'consistency': 2}\n",
      "Processed narrative 661: Consistency score = 2\n",
      "Overall progress: 1056/1056\n",
      "Combined run completed\n"
     ]
    }
   ],
   "source": [
    "model_queue = [\n",
    "    \"gemini-lite-structured\",\n",
    "]\n",
    "evaluaing_dataset_name = \"hanna\"\n",
    "\n",
    "for model_name in model_queue:\n",
    "    print(f\"Evaluating model: {model_name}\")\n",
    "    input_dataset_dir = os.path.join(data_dir, f\"{evaluaing_dataset_name}_stories.csv\")\n",
    "    await run_baseline_with_model(model_name, input_dataset_dir, evaluaing_dataset_name)\n",
    "    await run_fol_evaluator_with_model(model_name, input_dataset_dir, evaluaing_dataset_name)\n",
    "    await run_outline_evaluator_with_model(model_name, input_dataset_dir, evaluaing_dataset_name)\n",
    "    await run_character_evaluator_with_model(model_name, input_dataset_dir, evaluaing_dataset_name)\n",
    "    await run_combined_evaluator_with_model(model_name, input_dataset_dir, evaluaing_dataset_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
