{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d2ecc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import krippendorff\n",
    "\n",
    "from scipy.stats import kendalltau\n",
    "from sklearn.utils import resample\n",
    "\n",
    "cur_dir = \"C:/Users/Shavius/Documents/Uni/Year 4/Project/ELLMRPCTFVIS/dev\"\n",
    "data_dir = os.path.join(cur_dir, \"test_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25e2b24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Inter Rater Agreement\n",
    "\n",
    "def calculate_human_krippendorff(rater_dataset):\n",
    "    \n",
    "    score_cols = [col for col in rater_dataset.columns if col.startswith(\"human_score_\")]\n",
    "    score_matrix = rater_dataset[score_cols].values.T\n",
    "    \n",
    "    alpha_interval = krippendorff.alpha(reliability_data=score_matrix, level_of_measurement=\"interval\")\n",
    "    \n",
    "    return round(alpha_interval, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80205b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human Krippendorff's Alpha: -0.055\n"
     ]
    }
   ],
   "source": [
    "rater_dataset = pd.read_csv(os.path.join(data_dir, \"hanna_annotations_raters.csv\"))\n",
    "human_krippendorff = calculate_human_krippendorff(rater_dataset)\n",
    "print(f\"Human Krippendorff's Alpha: {human_krippendorff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2725ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_metric_correlations(human_scores, candidate_base, candidate_refined, n_bootsraps=1000, significance=0.05, rounding=3):\n",
    "    \n",
    "    if len(human_scores) == 0 or len(candidate_base) == 0 or len(candidate_refined) == 0:\n",
    "        raise ValueError(\"Input series must not be empty.\")\n",
    "    \n",
    "    if len(human_scores) != len(candidate_base) or len(human_scores) != len(candidate_refined):\n",
    "        raise ValueError(f\"Input series must have the same length.{len(human_scores)} != {len(candidate_base)}, {len(human_scores)} != {len(candidate_refined)}\")\n",
    "    \n",
    "    base_corr, _ = kendalltau(human_scores, candidate_base)\n",
    "    refined_corr, _ = kendalltau(human_scores, candidate_refined)\n",
    "    \n",
    "    n_obervations = len(human_scores)\n",
    "    data_pairs = np.column_stack((human_scores, candidate_base, candidate_refined))\n",
    "    \n",
    "    bootstrap_base_corrs = []\n",
    "    bootstrap_refined_corrs = []\n",
    "    \n",
    "    for i in range(n_bootsraps):\n",
    "        sample = resample(data_pairs, n_samples=n_obervations, replace=True)\n",
    "        \n",
    "        boot_base_corr, _ = kendalltau(sample[:, 0], sample[:, 1])\n",
    "        bootstrap_base_corrs.append(boot_base_corr)\n",
    "        \n",
    "        boot_refined_corr, _ = kendalltau(sample[:, 0], sample[:, 2])\n",
    "        bootstrap_refined_corrs.append(boot_refined_corr)\n",
    "    \n",
    "    ci_lower_base = np.percentile(bootstrap_base_corrs, 100 * significance / 2)\n",
    "    ci_upper_base = np.percentile(bootstrap_base_corrs, 100 * (1 - significance / 2))\n",
    "    \n",
    "    ci_lower_refined = np.percentile(bootstrap_refined_corrs, 100 * significance / 2)\n",
    "    ci_upper_refined = np.percentile(bootstrap_refined_corrs, 100 * (1 - significance / 2))\n",
    "    \n",
    "    base_corr = round(base_corr, rounding)\n",
    "    ci_range_base = round((ci_upper_base - ci_lower_base) / 2, rounding)\n",
    "    refined_corr = round(refined_corr, rounding)\n",
    "    ci_range_refined = round((ci_upper_refined - ci_lower_refined) / 2, rounding)\n",
    "    \n",
    "    return base_corr, ci_range_base, refined_corr, ci_range_refined\n",
    "    \n",
    "def evaluate_model_result(model_name, prefix, anno_type=\"average\"):\n",
    "    human_scores = pd.read_csv(os.path.join(data_dir, f\"{prefix}_annotations_{anno_type}.csv\"), index_col=0)[\"consistency\"].values\n",
    "    base_scores = pd.read_csv(os.path.join(data_dir, f\"{prefix}_baseline_output_{model_name}.csv\"), index_col=0)[\"consistency\"].values\n",
    "    refined_scores = pd.read_csv(os.path.join(data_dir, f\"{prefix}_combined_output_{model_name}.csv\"), index_col=0)[\"consistency\"].values\n",
    "    \n",
    "    base_corr, base_ci, refined_corr, refined_ci = calculate_metric_correlations(human_scores, base_scores, refined_scores, significance=0.05)\n",
    "\n",
    "    print(f\"Base model correlation: {base_corr}±{base_ci}\")\n",
    "    print(f\"Refined model correlation: {refined_corr}±{refined_ci}\")\n",
    "    return base_corr, refined_corr\n",
    "\n",
    "def evaluate_model_results_system_level(model_name, prefix, anno_type=\"average\"):\n",
    "    annotation = pd.read_csv(os.path.join(data_dir, f\"{prefix}_annotations_{anno_type}.csv\"), index_col=0).rename(columns={\"consistency\": \"human_score\"})\n",
    "    base_scores = pd.read_csv(os.path.join(data_dir, f\"{prefix}_baseline_output_{model_name}.csv\"), index_col=0).rename(columns={\"consistency\": \"base_score\"})\n",
    "    annotation = annotation.merge(base_scores, left_index=True, right_index=True)\n",
    "    \n",
    "    refined_scores = pd.read_csv(os.path.join(data_dir, f\"{prefix}_combined_output_{model_name}.csv\"), index_col=0).rename(columns={\"consistency\": \"refined_score\"})\n",
    "    annotation = annotation.merge(refined_scores, left_index=True, right_index=True)\n",
    "    \n",
    "    models = pd.read_csv(os.path.join(data_dir, f\"{prefix}_story_models.csv\"), index_col=0)\n",
    "    annotation = annotation.merge(models, left_index=True, right_index=True)\n",
    "    \n",
    "    annotation = annotation.groupby(\"model\").mean().reset_index()\n",
    "    \n",
    "    human_scores = annotation[\"human_score\"].values\n",
    "    base_scores = annotation[\"base_score\"].values\n",
    "    refined_scores = annotation[\"refined_score\"].values\n",
    "    \n",
    "    base_corr, base_ci, refined_corr, refined_ci = calculate_metric_correlations(human_scores, base_scores, refined_scores, significance=0.05)\n",
    "\n",
    "    print(f\"Base model correlation: {base_corr}±{base_ci}\")\n",
    "    print(f\"Refined model correlation: {refined_corr}±{refined_ci}\")\n",
    "    return base_corr, refined_corr\n",
    "\n",
    "def evaluate_model_results_different_raters(model_name, prefix):\n",
    "    human_scores = pd.read_csv(os.path.join(data_dir, f\"{prefix}_annotations_raters.csv\"), index_col=0)\n",
    "    base_scores = pd.read_csv(os.path.join(data_dir, f\"{prefix}_baseline_output_{model_name}.csv\"), index_col=0)\n",
    "    base_scores.rename(columns={\"consistency\": \"consistency_x\"}, inplace=True)\n",
    "    refined_scores = pd.read_csv(os.path.join(data_dir, f\"{prefix}_combined_output_{model_name}.csv\"), index_col=0)\n",
    "    refined_scores.rename(columns={\"consistency\": \"consistency_y\"}, inplace=True)\n",
    "    \n",
    "    base_corrs = []\n",
    "    refined_corrs = []\n",
    "    \n",
    "    for i in range(1, 5):\n",
    "        rater_scores = human_scores[[f\"human_score_{i}\"]]\n",
    "        rater_scores = rater_scores.merge(base_scores, left_index=True, right_index=True)\n",
    "        rater_scores = rater_scores.merge(refined_scores, left_index=True, right_index=True)\n",
    "        \n",
    "        rater_scores.dropna(inplace=True, subset=[f\"human_score_{i}\"])\n",
    "        if len(rater_scores) <= 1:\n",
    "            print(f\"Not enough data for rater {i}. Skipping...\")\n",
    "            continue\n",
    "        base_corr, _, refined_corr, _ = calculate_metric_correlations(\n",
    "            rater_scores[f\"human_score_{i}\"].values,\n",
    "            rater_scores[\"consistency_x\"].values,\n",
    "            rater_scores[\"consistency_y\"].values,\n",
    "            significance=0.05\n",
    "        )\n",
    "        \n",
    "        base_corr = 0.0 if np.isnan(base_corr) else base_corr.item()\n",
    "        refined_corr = 0.0 if np.isnan(refined_corr) else refined_corr.item()\n",
    "        \n",
    "        base_corrs.append(base_corr)\n",
    "        refined_corrs.append(refined_corr)\n",
    "    \n",
    "    mean_base_corr = round(np.mean(base_corrs), 3)\n",
    "    mean_refined_corr = round(np.mean(refined_corrs), 3)\n",
    "    print(f\"Base model rater correlations: mean: {mean_base_corr} max: {np.max(base_corrs)} min: {np.min(base_corrs)}\")\n",
    "    print(f\"Refined model rater correlations: mean: {mean_refined_corr} max: {np.max(refined_corrs)} min: {np.min(refined_corrs)}\")\n",
    "    return base_corrs, mean_base_corr, refined_corrs, mean_refined_corr\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d81c4f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model: gemini-structured\n",
      "Base model correlation: 0.224±0.054\n",
      "Refined model correlation: 0.232±0.047\n",
      "Base model correlation: 0.2±0.626\n",
      "Refined model correlation: 0.164±0.608\n",
      "Base model rater correlations: mean: 0.231 max: 0.309 min: 0.059\n",
      "Refined model rater correlations: mean: 0.229 max: 0.3 min: 0.077\n",
      "\n",
      "\n",
      "Evaluating model: gemini-15-structured\n",
      "Base model correlation: 0.378±0.049\n",
      "Refined model correlation: 0.271±0.051\n",
      "Base model correlation: 0.6±0.303\n",
      "Refined model correlation: 0.514±0.446\n",
      "Base model rater correlations: mean: 0.321 max: 0.39 min: 0.214\n",
      "Refined model rater correlations: mean: 0.266 max: 0.334 min: 0.124\n",
      "\n",
      "\n",
      "Evaluating model: gemini-lite-structured\n",
      "Base model correlation: 0.337±0.051\n",
      "Refined model correlation: 0.3±0.05\n",
      "Base model correlation: 0.537±0.429\n",
      "Refined model correlation: 0.564±0.308\n",
      "Base model rater correlations: mean: 0.299 max: 0.391 min: 0.124\n",
      "Refined model rater correlations: mean: 0.283 max: 0.34 min: 0.14\n",
      "\n",
      "\n",
      "Evaluating model: deepseek-structured\n",
      "Base model correlation: 0.363±0.045\n",
      "Refined model correlation: 0.265±0.047\n",
      "Base model correlation: 0.345±0.512\n",
      "Refined model correlation: 0.2±0.638\n",
      "Base model rater correlations: mean: 0.328 max: 0.415 min: 0.114\n",
      "Refined model rater correlations: mean: 0.27 max: 0.392 min: 0.104\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_names = [\"gemini-structured\", \"gemini-15-structured\", \"gemini-lite-structured\", \"deepseek-structured\"]\n",
    "for model_name in model_names:\n",
    "    print(f\"Evaluating model: {model_name}\")\n",
    "    evaluate_model_result(model_name, \"hanna\", anno_type=\"average\")\n",
    "    evaluate_model_results_system_level(model_name, \"hanna\", anno_type=\"average\")\n",
    "    evaluate_model_results_different_raters(model_name, \"hanna\")\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
