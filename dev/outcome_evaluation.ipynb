{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d2ecc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import krippendorff\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.utils import resample\n",
    "\n",
    "cur_dir = \"C:/Users/Shavius/Documents/Uni/Year 4/Project/ELLMRPCTFVIS/dev\"\n",
    "data_dir = os.path.join(cur_dir, \"test_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e2b24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Inter Rater Agreement\n",
    "\n",
    "def calculate_human_krippendorff(rater_dataset):\n",
    "    \n",
    "    score_cols = [col for col in rater_dataset.columns if col.startswith(\"human_score_\")]\n",
    "    score_matrix = rater_dataset[score_cols].values.T\n",
    "    \n",
    "    alpha_interval = krippendorff.alpha(reliability_data=score_matrix, level_of_measurement=\"interval\")\n",
    "    \n",
    "    return round(alpha_interval, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80205b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human Krippendorff's Alpha: 0.538\n"
     ]
    }
   ],
   "source": [
    "rater_dataset = pd.read_csv(os.path.join(data_dir, \"hanna_annotations_raters.csv\"))\n",
    "human_krippendorff = calculate_human_krippendorff(rater_dataset)\n",
    "print(f\"Human Krippendorff's Alpha: {human_krippendorff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2725ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_metric_correlations(human_scores, candidate_base, candidate_refined, n_bootsraps=1000, significance=0.05, rounding=3):\n",
    "    \n",
    "    if len(human_scores) == 0 or len(candidate_base) == 0 or len(candidate_refined) == 0:\n",
    "        raise ValueError(\"Input series must not be empty.\")\n",
    "    \n",
    "    if len(human_scores) != len(candidate_base) or len(human_scores) != len(candidate_refined):\n",
    "        raise ValueError(f\"Input series must have the same length.{len(human_scores)} != {len(candidate_base)}, {len(human_scores)} != {len(candidate_refined)}\")\n",
    "    \n",
    "    base_corr, _ = spearmanr(human_scores, candidate_base)\n",
    "    refined_corr, _ = spearmanr(human_scores, candidate_refined)\n",
    "    \n",
    "    n_obervations = len(human_scores)\n",
    "    data_pairs = np.column_stack((human_scores, candidate_base, candidate_refined))\n",
    "    \n",
    "    bootstrap_base_corrs = []\n",
    "    bootstrap_refined_corrs = []\n",
    "    \n",
    "    for i in range(n_bootsraps):\n",
    "        sample = resample(data_pairs, n_samples=n_obervations, replace=True)\n",
    "        \n",
    "        boot_base_corr, _ = spearmanr(sample[:, 0], sample[:, 1])\n",
    "        bootstrap_base_corrs.append(boot_base_corr)\n",
    "        \n",
    "        boot_refined_corr, _ = spearmanr(sample[:, 0], sample[:, 2])\n",
    "        bootstrap_refined_corrs.append(boot_refined_corr)\n",
    "    \n",
    "    ci_lower_base = np.percentile(bootstrap_base_corrs, 100 * significance / 2)\n",
    "    ci_upper_base = np.percentile(bootstrap_base_corrs, 100 * (1 - significance / 2))\n",
    "    \n",
    "    ci_lower_refined = np.percentile(bootstrap_refined_corrs, 100 * significance / 2)\n",
    "    ci_upper_refined = np.percentile(bootstrap_refined_corrs, 100 * (1 - significance / 2))\n",
    "    \n",
    "    base_corr = round(base_corr, rounding)\n",
    "    ci_lower_base = round(ci_lower_base, rounding)\n",
    "    ci_upper_base = round(ci_upper_base, rounding)\n",
    "    refined_corr = round(refined_corr, rounding)\n",
    "    ci_lower_refined = round(ci_lower_refined, rounding)\n",
    "    ci_upper_refined = round(ci_upper_refined, rounding)\n",
    "    \n",
    "    return base_corr, (ci_lower_base, ci_upper_base), refined_corr, (ci_lower_refined, ci_upper_refined)\n",
    "    \n",
    "def evaluate_model_result(model_name, prefix, anno_type=\"average\"):\n",
    "    human_scores = pd.read_csv(os.path.join(data_dir, f\"{prefix}_annotations_{anno_type}.csv\"), index_col=0)[\"consistency\"].values\n",
    "    base_scores = pd.read_csv(os.path.join(data_dir, f\"{prefix}_baseline_output_{model_name}.csv\"), index_col=0)[\"consistency\"].values\n",
    "    refined_scores = pd.read_csv(os.path.join(data_dir, f\"{prefix}_combined_output_{model_name}.csv\"), index_col=0)[\"consistency\"].values\n",
    "    \n",
    "    base_corr, (base_lower_ci, base_upper_ci), refined_corr, (refined_lower_ci, refined_upper_ci) = calculate_metric_correlations(human_scores, base_scores, refined_scores, significance=0.05)\n",
    "\n",
    "    print(f\"Base model correlation: {base_corr} ({base_lower_ci}-{base_upper_ci})\")\n",
    "    print(f\"Refined model correlation: {refined_corr} ({refined_lower_ci}-{refined_upper_ci})\")\n",
    "    return base_corr, refined_corr, (base_lower_ci, base_upper_ci), (refined_lower_ci, refined_upper_ci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d81c4f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model: gemini-structured\n",
      "Base model correlation: 0.293 (0.228-0.354)\n",
      "Refined model correlation: 0.304 (0.243-0.36)\n",
      "\n",
      "\n",
      "Evaluating model: gemini-15-structured\n",
      "Base model correlation: 0.43 (0.371-0.488)\n",
      "Refined model correlation: 0.358 (0.294-0.414)\n",
      "\n",
      "\n",
      "Evaluating model: gemini-lite-structured\n",
      "Base model correlation: 0.428 (0.371-0.48)\n",
      "Refined model correlation: 0.359 (0.306-0.414)\n",
      "\n",
      "\n",
      "Evaluating model: deepseek-structured\n",
      "Base model correlation: 0.423 (0.366-0.478)\n",
      "Refined model correlation: 0.318 (0.264-0.377)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_names = [\"gemini-structured\", \"gemini-15-structured\", \"gemini-lite-structured\", \"deepseek-structured\"]\n",
    "for model_name in model_names:\n",
    "    print(f\"Evaluating model: {model_name}\")\n",
    "    evaluate_model_result(model_name, \"hanna\", anno_type=\"average\")\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
