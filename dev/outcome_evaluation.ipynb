{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d2ecc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import krippendorff\n",
    "\n",
    "from scipy.stats import kendalltau\n",
    "from sklearn.utils import resample\n",
    "\n",
    "cur_dir = \"C:/Users/Shavius/Documents/Uni/Year 4/Project/ELLMRPCTFVIS/dev\"\n",
    "data_dir = os.path.join(cur_dir, \"test_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25e2b24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Inter Rater Agreement\n",
    "\n",
    "def calculate_human_krippendorff(rater_dataset):\n",
    "    \n",
    "    score_cols = [col for col in rater_dataset.columns if col.startswith(\"human_score_\")]\n",
    "    score_matrix = rater_dataset[score_cols].values.T\n",
    "    \n",
    "    alpha_interval = krippendorff.alpha(reliability_data=score_matrix, level_of_measurement=\"interval\")\n",
    "    \n",
    "    return round(alpha_interval, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80205b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human Krippendorff's Alpha: -0.055\n"
     ]
    }
   ],
   "source": [
    "rater_dataset = pd.read_csv(os.path.join(data_dir, \"hanna_annotations_raters.csv\"))\n",
    "human_krippendorff = calculate_human_krippendorff(rater_dataset)\n",
    "print(f\"Human Krippendorff's Alpha: {human_krippendorff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2725ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_metric_correlations(human_scores, candidate_base, candidate_refined, n_bootsraps=1000, significance=0.05, rounding=3):\n",
    "    \n",
    "    if len(human_scores) == 0 or len(candidate_base) == 0 or len(candidate_refined) == 0:\n",
    "        raise ValueError(\"Input series must not be empty.\")\n",
    "    \n",
    "    if len(human_scores) != len(candidate_base) or len(human_scores) != len(candidate_refined):\n",
    "        raise ValueError(f\"Input series must have the same length.{len(human_scores)} != {len(candidate_base)}, {len(human_scores)} != {len(candidate_refined)}\")\n",
    "    \n",
    "    base_corr, _ = kendalltau(human_scores, candidate_base)\n",
    "    refined_corr, _ = kendalltau(human_scores, candidate_refined)\n",
    "    \n",
    "    n_obervations = len(human_scores)\n",
    "    data_pairs = np.column_stack((human_scores, candidate_base, candidate_refined))\n",
    "    \n",
    "    bootstrap_base_corrs = []\n",
    "    bootstrap_refined_corrs = []\n",
    "    \n",
    "    for i in range(n_bootsraps):\n",
    "        sample = resample(data_pairs, n_samples=n_obervations, replace=True)\n",
    "        \n",
    "        boot_base_corr, _ = kendalltau(sample[:, 0], sample[:, 1])\n",
    "        bootstrap_base_corrs.append(boot_base_corr)\n",
    "        \n",
    "        boot_refined_corr, _ = kendalltau(sample[:, 0], sample[:, 2])\n",
    "        bootstrap_refined_corrs.append(boot_refined_corr)\n",
    "    \n",
    "    ci_lower_base = np.percentile(bootstrap_base_corrs, 100 * significance / 2)\n",
    "    ci_upper_base = np.percentile(bootstrap_base_corrs, 100 * (1 - significance / 2))\n",
    "    \n",
    "    ci_lower_refined = np.percentile(bootstrap_refined_corrs, 100 * significance / 2)\n",
    "    ci_upper_refined = np.percentile(bootstrap_refined_corrs, 100 * (1 - significance / 2))\n",
    "    \n",
    "    base_corr = round(base_corr, rounding)\n",
    "    ci_lower_base = round(ci_lower_base, rounding)\n",
    "    ci_upper_base = round(ci_upper_base, rounding)\n",
    "    refined_corr = round(refined_corr, rounding)\n",
    "    ci_lower_refined = round(ci_lower_refined, rounding)\n",
    "    ci_upper_refined = round(ci_upper_refined, rounding)\n",
    "    \n",
    "    return base_corr, (ci_lower_base, ci_upper_base), refined_corr, (ci_lower_refined, ci_upper_refined)\n",
    "    \n",
    "def evaluate_model_result(model_name, prefix, anno_type=\"average\"):\n",
    "    human_scores = pd.read_csv(os.path.join(data_dir, f\"{prefix}_annotations_{anno_type}.csv\"), index_col=0)[\"consistency\"].values\n",
    "    base_scores = pd.read_csv(os.path.join(data_dir, f\"{prefix}_baseline_output_{model_name}.csv\"), index_col=0)[\"consistency\"].values\n",
    "    refined_scores = pd.read_csv(os.path.join(data_dir, f\"{prefix}_combined_output_{model_name}.csv\"), index_col=0)[\"consistency\"].values\n",
    "    \n",
    "    base_corr, (base_lower_ci, base_upper_ci), refined_corr, (refined_lower_ci, refined_upper_ci) = calculate_metric_correlations(human_scores, base_scores, refined_scores, significance=0.05)\n",
    "\n",
    "    print(f\"Base model correlation: {base_corr} ({base_lower_ci}-{base_upper_ci})\")\n",
    "    print(f\"Refined model correlation: {refined_corr} ({refined_lower_ci}-{refined_upper_ci})\")\n",
    "    return base_corr, refined_corr, (base_lower_ci, base_upper_ci), (refined_lower_ci, refined_upper_ci)\n",
    "\n",
    "def evaluate_model_results_system_level(model_name, prefix, anno_type=\"average\"):\n",
    "    annotation = pd.read_csv(os.path.join(data_dir, f\"{prefix}_annotations_{anno_type}.csv\"), index_col=0).rename(columns={\"consistency\": \"human_score\"})\n",
    "    base_scores = pd.read_csv(os.path.join(data_dir, f\"{prefix}_baseline_output_{model_name}.csv\"), index_col=0).rename(columns={\"consistency\": \"base_score\"})\n",
    "    annotation = annotation.merge(base_scores, left_index=True, right_index=True)\n",
    "    \n",
    "    refined_scores = pd.read_csv(os.path.join(data_dir, f\"{prefix}_combined_output_{model_name}.csv\"), index_col=0).rename(columns={\"consistency\": \"refined_score\"})\n",
    "    annotation = annotation.merge(refined_scores, left_index=True, right_index=True)\n",
    "    \n",
    "    models = pd.read_csv(os.path.join(data_dir, f\"{prefix}_story_models.csv\"), index_col=0)\n",
    "    annotation = annotation.merge(models, left_index=True, right_index=True)\n",
    "    \n",
    "    annotation = annotation.groupby(\"model\").mean().reset_index()\n",
    "    \n",
    "    human_scores = annotation[\"human_score\"].values\n",
    "    base_scores = annotation[\"base_score\"].values\n",
    "    refined_scores = annotation[\"refined_score\"].values\n",
    "    \n",
    "    base_corr, (base_lower_ci, base_upper_ci), refined_corr, (refined_lower_ci, refined_upper_ci) = calculate_metric_correlations(human_scores, base_scores, refined_scores, significance=0.05)\n",
    "    print(f\"Base model system-level correlation: {base_corr} ({base_lower_ci}-{base_upper_ci})\")\n",
    "    print(f\"Refined model system-level correlation: {refined_corr} ({refined_lower_ci}-{refined_upper_ci})\")\n",
    "    return base_corr, refined_corr, (base_lower_ci, base_upper_ci), (refined_lower_ci, refined_upper_ci)\n",
    "\n",
    "def evaluate_model_results_different_raters(model_name, prefix):\n",
    "    human_scores = pd.read_csv(os.path.join(data_dir, f\"{prefix}_annotations_raters.csv\"), index_col=0)\n",
    "    base_scores = pd.read_csv(os.path.join(data_dir, f\"{prefix}_baseline_output_{model_name}.csv\"), index_col=0)\n",
    "    base_scores.rename(columns={\"consistency\": \"consistency_x\"}, inplace=True)\n",
    "    refined_scores = pd.read_csv(os.path.join(data_dir, f\"{prefix}_combined_output_{model_name}.csv\"), index_col=0)\n",
    "    refined_scores.rename(columns={\"consistency\": \"consistency_y\"}, inplace=True)\n",
    "    \n",
    "    base_corrs = []\n",
    "    refined_corrs = []\n",
    "    \n",
    "    for i in range(1, 5):\n",
    "        rater_scores = human_scores[[f\"human_score_{i}\"]]\n",
    "        rater_scores = rater_scores.merge(base_scores, left_index=True, right_index=True)\n",
    "        rater_scores = rater_scores.merge(refined_scores, left_index=True, right_index=True)\n",
    "        \n",
    "        rater_scores.dropna(inplace=True, subset=[f\"human_score_{i}\"])\n",
    "        if len(rater_scores) <= 1:\n",
    "            print(f\"Not enough data for rater {i}. Skipping...\")\n",
    "            continue\n",
    "        base_corr, _, refined_corr, _ = calculate_metric_correlations(\n",
    "            rater_scores[f\"human_score_{i}\"].values,\n",
    "            rater_scores[\"consistency_x\"].values,\n",
    "            rater_scores[\"consistency_y\"].values,\n",
    "            significance=0.05\n",
    "        )\n",
    "        \n",
    "        base_corr = 0.0 if np.isnan(base_corr) else base_corr.item()\n",
    "        refined_corr = 0.0 if np.isnan(refined_corr) else refined_corr.item()\n",
    "        \n",
    "        base_corrs.append(base_corr)\n",
    "        refined_corrs.append(refined_corr)\n",
    "    \n",
    "    print(f\"Base model rater correlations: mean: {np.mean(base_corrs)} max: {np.max(base_corrs)} min: {np.min(base_corrs)}\")\n",
    "    print(f\"Refined model rater correlations: mean: {np.mean(refined_corrs)} max: {np.max(refined_corrs)} min: {np.min(refined_corrs)}\")\n",
    "    return base_corrs, np.mean(base_corrs), refined_corrs, np.mean(refined_corrs)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d81c4f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model: gemini-structured\n",
      "Base model correlation: 0.247 (0.191-0.302)\n",
      "Refined model correlation: 0.252 (0.204-0.3)\n",
      "Base model system-level correlation: 0.273 (-0.391-0.84)\n",
      "Refined model system-level correlation: 0.127 (-0.52-0.692)\n",
      "Base model rater correlations: mean: 0.23775000000000002 max: 0.306 min: 0.067\n",
      "Refined model rater correlations: mean: 0.25075000000000003 max: 0.337 min: 0.077\n",
      "\n",
      "\n",
      "Evaluating model: gemini-15-structured\n",
      "Base model correlation: 0.36 (0.312-0.409)\n",
      "Refined model correlation: 0.305 (0.255-0.355)\n",
      "Base model system-level correlation: 0.6 (0.304-0.915)\n",
      "Refined model system-level correlation: 0.33 (-0.269-0.84)\n",
      "Base model rater correlations: mean: 0.33475 max: 0.435 min: 0.186\n",
      "Refined model rater correlations: mean: 0.273 max: 0.357 min: 0.066\n",
      "\n",
      "\n",
      "Evaluating model: gemini-lite-structured\n",
      "Base model correlation: 0.358 (0.313-0.406)\n",
      "Refined model correlation: 0.31 (0.262-0.361)\n",
      "Base model system-level correlation: 0.527 (0.182-0.837)\n",
      "Refined model system-level correlation: 0.6 (0.208-0.917)\n",
      "Base model rater correlations: mean: 0.3305 max: 0.406 min: 0.14\n",
      "Refined model rater correlations: mean: 0.26625 max: 0.329 min: 0.156\n",
      "\n",
      "\n",
      "Evaluating model: deepseek-structured\n",
      "Base model correlation: 0.363 (0.315-0.41)\n",
      "Refined model correlation: 0.265 (0.211-0.314)\n",
      "Base model system-level correlation: 0.345 (-0.231-0.83)\n",
      "Refined model system-level correlation: 0.2 (-0.462-0.792)\n",
      "Base model rater correlations: mean: 0.3285 max: 0.415 min: 0.114\n",
      "Refined model rater correlations: mean: 0.27025 max: 0.392 min: 0.104\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_names = [\"gemini-structured\", \"gemini-15-structured\", \"gemini-lite-structured\", \"deepseek-structured\"]\n",
    "for model_name in model_names:\n",
    "    print(f\"Evaluating model: {model_name}\")\n",
    "    evaluate_model_result(model_name, \"hanna\", anno_type=\"average\")\n",
    "    evaluate_model_results_system_level(model_name, \"hanna\", anno_type=\"average\")\n",
    "    evaluate_model_results_different_raters(model_name, \"hanna\")\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
